{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61d7ad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd54bb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Badr\\AppData\\Local\\Temp\\ipykernel_15956\\3373649270.py:1: DtypeWarning: Columns (0,3,5,6,7,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"twitter_human_bots_dataset_corrupted.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (37446, 20)\n",
      "\n",
      "Columns and types:\n",
      "Unnamed: 0                      object\n",
      "created_at                      object\n",
      "default_profile                 object\n",
      "default_profile_image           object\n",
      "description                     object\n",
      "favourites_count                object\n",
      "followers_count                 object\n",
      "friends_count                   object\n",
      "geo_enabled                     object\n",
      "id                              object\n",
      "lang                            object\n",
      "location                        object\n",
      "profile_background_image_url    object\n",
      "profile_image_url               object\n",
      "screen_name                     object\n",
      "statuses_count                  object\n",
      "verified                        object\n",
      "average_tweets_per_day          object\n",
      "account_age_days                object\n",
      "account_type                    object\n",
      "dtype: object\n",
      "\n",
      "First few rows:\n",
      "  Unnamed: 0           created_at default_profile default_profile_image  \\\n",
      "0          0  2016-10-15 21:32:11           False                 False   \n",
      "1          1  2016-11-09 05:01:30           False                 False   \n",
      "2          2  2017-06-17 05:34:27           False                 False   \n",
      "3          3  2016-07-21 13:32:25            True                 False   \n",
      "4          4  2012-01-15 16:32:35           False                 False   \n",
      "\n",
      "                                         description favourites_count  \\\n",
      "0  Blame xaiax, Inspired by MakingInvisible, usin...                4   \n",
      "1  Photographing the American West since 1980. I ...              536   \n",
      "2  Scruffy looking nerf herder and twitch broadca...             3307   \n",
      "3  Wife.Godmother.Friend.Feline Fanatic! Assistan...             8433   \n",
      "4                Loan coach at mancity & Aspiring DJ               88   \n",
      "\n",
      "  followers_count friends_count geo_enabled                  id lang  \\\n",
      "0            1589             4       False  787405734442958848   en   \n",
      "1             860           880       False  796216118331310080   en   \n",
      "2             172           594        True  875949740503859204   en   \n",
      "3             517           633        True  756119643622735875   en   \n",
      "4          753678           116        True           464781334   en   \n",
      "\n",
      "                  location                      profile_background_image_url  \\\n",
      "0                  unknown  http://abs.twimg.com/images/themes/theme1/bg.png   \n",
      "1           Estados Unidos  http://abs.twimg.com/images/themes/theme1/bg.png   \n",
      "2          Los Angeles, CA  http://abs.twimg.com/images/themes/theme1/bg.png   \n",
      "3           Birmingham, AL                                               NaN   \n",
      "4  England, United Kingdom  http://abs.twimg.com/images/themes/theme1/bg.png   \n",
      "\n",
      "                                   profile_image_url      screen_name  \\\n",
      "0  http://pbs.twimg.com/profile_images/7874121826...  best_in_dumbest   \n",
      "1  http://pbs.twimg.com/profile_images/8023296328...     CJRubinPhoto   \n",
      "2  http://pbs.twimg.com/profile_images/1278890453...         SVGEGENT   \n",
      "3  http://pbs.twimg.com/profile_images/1284884924...    TinkerVHELPK5   \n",
      "4  http://pbs.twimg.com/profile_images/9952566258...    JoleonLescott   \n",
      "\n",
      "  statuses_count verified average_tweets_per_day account_age_days account_type  \n",
      "0          11041    False                   7.87             1403          bot  \n",
      "1            252    False                  0.183             1379        human  \n",
      "2           1001    False                  0.864             1159        human  \n",
      "3           1324    False                  0.889             1489        human  \n",
      "4           4202     True                  1.339             3138        human  \n",
      "\n",
      "Basic stats:\n",
      "       Unnamed: 0           created_at default_profile default_profile_image  \\\n",
      "count       37438                37438           37433                 37437   \n",
      "unique      37437                37430              14                    14   \n",
      "top         11282  2015-11-03 10:56:15           False                 False   \n",
      "freq            2                    2           21704                 32243   \n",
      "\n",
      "       description favourites_count followers_count friends_count geo_enabled  \\\n",
      "count        30175            37432           37433         37431       37433   \n",
      "unique       29936            17071           14262          6775          16   \n",
      "top              .                0               0             0       False   \n",
      "freq            14              748            1100          5585       20343   \n",
      "\n",
      "            id   lang location  \\\n",
      "count    37434  29473    37426   \n",
      "unique   37431     58    12043   \n",
      "top     999999     en  unknown   \n",
      "freq         4  21430    13019   \n",
      "\n",
      "                            profile_background_image_url  \\\n",
      "count                                              32938   \n",
      "unique                                                34   \n",
      "top     http://abs.twimg.com/images/themes/theme1/bg.png   \n",
      "freq                                               21806   \n",
      "\n",
      "                                        profile_image_url screen_name  \\\n",
      "count                                               37432       37436   \n",
      "unique                                              36875       37434   \n",
      "top     http://abs.twimg.com/sticky/default_profile_im...     unknown   \n",
      "freq                                                  558           2   \n",
      "\n",
      "       statuses_count verified average_tweets_per_day account_age_days  \\\n",
      "count           37434    37427                  37424            37430   \n",
      "unique          19191       12                  14945             4167   \n",
      "top              1332    False                  0.004             1155   \n",
      "freq               67    29861                     75              253   \n",
      "\n",
      "       account_type  \n",
      "count         37427  \n",
      "unique            7  \n",
      "top           human  \n",
      "freq          25002  \n",
      "\n",
      "Missing values:\n",
      "Unnamed: 0                         8\n",
      "created_at                         8\n",
      "default_profile                   13\n",
      "default_profile_image              9\n",
      "description                     7271\n",
      "favourites_count                  14\n",
      "followers_count                   13\n",
      "friends_count                     15\n",
      "geo_enabled                       13\n",
      "id                                12\n",
      "lang                            7973\n",
      "location                          20\n",
      "profile_background_image_url    4508\n",
      "profile_image_url                 14\n",
      "screen_name                       10\n",
      "statuses_count                    12\n",
      "verified                          19\n",
      "average_tweets_per_day            22\n",
      "account_age_days                  16\n",
      "account_type                      19\n",
      "dtype: int64\n",
      "account_type\n",
      "human          25002\n",
      "bot            12418\n",
      "INVALID_bot        2\n",
      "HUMAN              2\n",
      "7JQFi              1\n",
      "namuh              1\n",
      "teW                1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"twitter_human_bots_dataset_corrupted.csv\")\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"\\nColumns and types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nBasic stats:\")\n",
    "print(df.describe())\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(df['account_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c45d240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value Counts:\n",
      "account_type\n",
      "human          25002\n",
      "bot            12418\n",
      "INVALID_bot        2\n",
      "HUMAN              2\n",
      "7JQFi              1\n",
      "namuh              1\n",
      "teW                1\n",
      "Name: count, dtype: int64\n",
      "Missing values: 19\n",
      "Total rows: 37446\n",
      "\n",
      "==================================================\n",
      "CLEANING ACTIONS:\n",
      "==================================================\n",
      "- Standardized all values to lowercase\n",
      "- Mapped 'invalid_bot' -> 'bot'\n",
      "- Mapped 'namuh' (backwards) -> 'human'\n",
      "- Set clearly invalid values ('7jqfi', 'tew') to NaN\n",
      "- Dropping all rows with NaN in account_type\n",
      "\n",
      "==================================================\n",
      "AFTER CLEANING:\n",
      "==================================================\n",
      "Total rows: 37425\n",
      "Rows dropped: 21\n",
      "\n",
      "Value counts:\n",
      "account_type\n",
      "human    25005\n",
      "bot      12420\n",
      "Name: count, dtype: int64\n",
      "Missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# cleaning label\n",
    "print(\"\\nValue Counts:\")\n",
    "print(df['account_type'].value_counts())\n",
    "\n",
    "print(f\"Missing values: {df['account_type'].isna().sum()}\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "\n",
    "# Step 1: Standardize to lowercase\n",
    "df['account_type'] = df['account_type'].str.lower().str.strip()\n",
    "\n",
    "# Step 2: Map variations to standard values\n",
    "mapping = {\n",
    "    'human': 'human',\n",
    "    'bot': 'bot',\n",
    "    'invalid_bot': 'bot',  \n",
    "    '7jqfi': np.nan,       \n",
    "    'namuh': 'human',      \n",
    "    'tew': np.nan          \n",
    "}\n",
    "\n",
    "df['account_type'] = df['account_type'].map(mapping)\n",
    "\n",
    "# drop nan\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CLEANING ACTIONS:\")\n",
    "print(\"=\"*50)\n",
    "print(\"- Standardized all values to lowercase\")\n",
    "print(\"- Mapped 'invalid_bot' -> 'bot'\")\n",
    "print(\"- Mapped 'namuh' (backwards) -> 'human'\")\n",
    "print(\"- Set clearly invalid values ('7jqfi', 'tew') to NaN\")\n",
    "print(\"- Dropping all rows with NaN in account_type\")\n",
    "\n",
    "df_clean = df.dropna(subset=['account_type'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"AFTER CLEANING:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total rows: {len(df_clean)}\")\n",
    "print(f\"Rows dropped: {len(df) - len(df_clean)}\")\n",
    "print(f\"\\nValue counts:\\n{df_clean['account_type'].value_counts()}\")\n",
    "print(f\"Missing values: {df_clean['account_type'].isna().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28f777cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXAMINING BOOLEAN FEATURES\n",
      "======================================================================\n",
      "Total rows: 37446\n",
      "\n",
      "======================================================================\n",
      "COLUMN: default_profile\n",
      "======================================================================\n",
      "Data type: object\n",
      "\n",
      "Unique values: 14\n",
      "\n",
      "Value counts:\n",
      "default_profile\n",
      "False      21704\n",
      "True       15706\n",
      "NaN           13\n",
      "eslaF          7\n",
      "eurT           5\n",
      "unknown        2\n",
      "0              1\n",
      "-9999          1\n",
      "161            1\n",
      "FeNf5          1\n",
      "3706           1\n",
      "FALSE          1\n",
      "TRUE           1\n",
      "676            1\n",
      "B01X           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentage distribution:\n",
      "default_profile\n",
      "False      57.960797\n",
      "True       41.943065\n",
      "NaN         0.034717\n",
      "eslaF       0.018694\n",
      "eurT        0.013353\n",
      "unknown     0.005341\n",
      "0           0.002671\n",
      "-9999       0.002671\n",
      "161         0.002671\n",
      "FeNf5       0.002671\n",
      "3706        0.002671\n",
      "FALSE       0.002671\n",
      "TRUE        0.002671\n",
      "676         0.002671\n",
      "B01X        0.002671\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Missing values: 13 (0.03%)\n",
      "\n",
      "⚠️  WARNING: Column is stored as object/string, not boolean\n",
      "Sample values: ['False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'True', 'False']\n",
      "\n",
      "\n",
      "======================================================================\n",
      "COLUMN: default_profile_image\n",
      "======================================================================\n",
      "Data type: object\n",
      "\n",
      "Unique values: 14\n",
      "\n",
      "Value counts:\n",
      "default_profile_image\n",
      "False            32243\n",
      "False             4615\n",
      "True               495\n",
      "True                63\n",
      "NaN                  9\n",
      "FALSE                4\n",
      "0                    4\n",
      "eslaF                4\n",
      "-9999                2\n",
      "INVALID_False        2\n",
      "JuaP6                1\n",
      "GHOdo                1\n",
      "unknown              1\n",
      "999999               1\n",
      "lsUB6                1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentage distribution:\n",
      "default_profile_image\n",
      "False            86.105325\n",
      "False            12.324414\n",
      "True              1.321904\n",
      "True              0.168242\n",
      "NaN               0.024035\n",
      "FALSE             0.010682\n",
      "0                 0.010682\n",
      "eslaF             0.010682\n",
      "-9999             0.005341\n",
      "INVALID_False     0.005341\n",
      "JuaP6             0.002671\n",
      "GHOdo             0.002671\n",
      "unknown           0.002671\n",
      "999999            0.002671\n",
      "lsUB6             0.002671\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Missing values: 9 (0.02%)\n",
      "\n",
      "⚠️  WARNING: Column is stored as object/string, not boolean\n",
      "Sample values: ['False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False']\n",
      "\n",
      "\n",
      "======================================================================\n",
      "COLUMN: verified\n",
      "======================================================================\n",
      "Data type: object\n",
      "\n",
      "Unique values: 12\n",
      "\n",
      "Value counts:\n",
      "verified\n",
      "False            29861\n",
      "True              7545\n",
      "NaN                 19\n",
      "FALSE                5\n",
      "eslaF                4\n",
      "INVALID_False        3\n",
      "INVALID_True         2\n",
      "eurT                 2\n",
      "999999               1\n",
      "61q3S                1\n",
      "OtnX                 1\n",
      "-9999                1\n",
      "unknown              1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentage distribution:\n",
      "verified\n",
      "False            79.744165\n",
      "True             20.149015\n",
      "NaN               0.050740\n",
      "FALSE             0.013353\n",
      "eslaF             0.010682\n",
      "INVALID_False     0.008012\n",
      "INVALID_True      0.005341\n",
      "eurT              0.005341\n",
      "999999            0.002671\n",
      "61q3S             0.002671\n",
      "OtnX              0.002671\n",
      "-9999             0.002671\n",
      "unknown           0.002671\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Missing values: 19 (0.05%)\n",
      "\n",
      "⚠️  WARNING: Column is stored as object/string, not boolean\n",
      "Sample values: ['False', 'False', 'False', 'False', 'True', 'False', 'True', 'True', 'False', 'False']\n",
      "\n",
      "\n",
      "======================================================================\n",
      "COLUMN: geo_enabled\n",
      "======================================================================\n",
      "Data type: object\n",
      "\n",
      "Unique values: 16\n",
      "\n",
      "Value counts:\n",
      "geo_enabled\n",
      "False                                                20343\n",
      "True                                                 17058\n",
      "NaN                                                     13\n",
      "eslaF                                                    7\n",
      "TRUE                                                     4\n",
      "eurT                                                     3\n",
      "unknown                                                  3\n",
      "999999                                                   3\n",
      "FALSE                                                    2\n",
      "-9999                                                    2\n",
      "http://abs.twimg.com/images/themes/theme1/bg.png         2\n",
      "http://abs.twimg.com/images/themes/theme10/bg.gif        1\n",
      "QHAQ                                                     1\n",
      "INVALID_False                                            1\n",
      "INVALID_True                                             1\n",
      "Lh1F                                                     1\n",
      "KnR4                                                     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentage distribution:\n",
      "geo_enabled\n",
      "False                                                54.326230\n",
      "True                                                 45.553597\n",
      "NaN                                                   0.034717\n",
      "eslaF                                                 0.018694\n",
      "TRUE                                                  0.010682\n",
      "eurT                                                  0.008012\n",
      "unknown                                               0.008012\n",
      "999999                                                0.008012\n",
      "FALSE                                                 0.005341\n",
      "-9999                                                 0.005341\n",
      "http://abs.twimg.com/images/themes/theme1/bg.png      0.005341\n",
      "http://abs.twimg.com/images/themes/theme10/bg.gif     0.002671\n",
      "QHAQ                                                  0.002671\n",
      "INVALID_False                                         0.002671\n",
      "INVALID_True                                          0.002671\n",
      "Lh1F                                                  0.002671\n",
      "KnR4                                                  0.002671\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Missing values: 13 (0.03%)\n",
      "\n",
      "⚠️  WARNING: Column is stored as object/string, not boolean\n",
      "Sample values: ['False', 'False', 'True', 'True', 'True', 'False', 'True', 'False', 'False', 'True']\n",
      "\n",
      "\n",
      "======================================================================\n",
      "SUMMARY & RECOMMENDATIONS\n",
      "======================================================================\n",
      "\n",
      "Next steps:\n",
      "1. Check if values need to be converted to proper boolean type\n",
      "2. Decide how to handle missing values\n",
      "3. Look for any unexpected values (e.g., 'True' as string vs True as bool)\n"
     ]
    }
   ],
   "source": [
    "# Examine Boolean Features\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXAMINING BOOLEAN FEATURES\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total rows: {len(df)}\\n\")\n",
    "\n",
    "# Boolean columns to examine\n",
    "boolean_cols = ['default_profile', 'default_profile_image', 'verified', 'geo_enabled']\n",
    "\n",
    "for col in boolean_cols:\n",
    "    print(\"=\"*70)\n",
    "    print(f\"COLUMN: {col}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Show data type\n",
    "    print(f\"Data type: {df[col].dtype}\")\n",
    "    \n",
    "    # Show unique values\n",
    "    print(f\"\\nUnique values: {df[col].nunique()}\")\n",
    "    print(\"\\nValue counts:\")\n",
    "    print(df[col].value_counts(dropna=False))\n",
    "    \n",
    "    # Show percentage\n",
    "    print(\"\\nPercentage distribution:\")\n",
    "    print(df[col].value_counts(normalize=True, dropna=False) * 100)\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing = df[col].isnull().sum()\n",
    "    print(f\"\\nMissing values: {missing} ({missing/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    # Check for non-boolean values (if stored as object/string)\n",
    "    if df[col].dtype == 'object':\n",
    "        print(\"\\n⚠️  WARNING: Column is stored as object/string, not boolean\")\n",
    "        print(\"Sample values:\", df[col].head(10).tolist())\n",
    "    \n",
    "    print(\"\\n\")\n",
    "\n",
    "# Summary\n",
    "print(\"=\"*70)\n",
    "print(\"SUMMARY & RECOMMENDATIONS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Check if values need to be converted to proper boolean type\")\n",
    "print(\"2. Decide how to handle missing values\")\n",
    "print(\"3. Look for any unexpected values (e.g., 'True' as string vs True as bool)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bae15c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CLEANING: default_profile\n",
      "======================================================================\n",
      "\n",
      "BEFORE CLEANING:\n",
      "Data type: object\n",
      "Unique values: 14\n",
      "\n",
      "Value counts:\n",
      "default_profile\n",
      "False      21704\n",
      "True       15706\n",
      "NaN           13\n",
      "eslaF          7\n",
      "eurT           5\n",
      "unknown        2\n",
      "0              1\n",
      "-9999          1\n",
      "161            1\n",
      "FeNf5          1\n",
      "3706           1\n",
      "FALSE          1\n",
      "TRUE           1\n",
      "676            1\n",
      "B01X           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "======================================================================\n",
      "AFTER CLEANING:\n",
      "======================================================================\n",
      "Data type: object\n",
      "Unique values: 2\n",
      "\n",
      "Value counts:\n",
      "default_profile\n",
      "False    21713\n",
      "True     15712\n",
      "NaN         21\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total missing/invalid values: 21 (0.06%)\n",
      "\n",
      "======================================================================\n",
      "CLEANING SUMMARY:\n",
      "======================================================================\n",
      "✓ Standardized 'True'/'TRUE'/'true' → True\n",
      "✓ Standardized 'False'/'FALSE'/'false' → False\n",
      "✓ Mapped 'eurT' (backwards) → True\n",
      "✓ Mapped 'eslaF' (backwards) → False\n",
      "✓ Set invalid values (numbers, random strings) → NaN\n",
      "\n",
      "Rows with True: 15712\n",
      "Rows with False: 21713\n",
      "Rows with NaN: 21\n",
      "\n",
      "======================================================================\n",
      "HANDLING MISSING VALUES:\n",
      "======================================================================\n",
      "Missing values before imputation: 21\n",
      "Most common value (mode): False\n",
      "Missing values after imputation: 0\n",
      "✓ All missing values imputed with False\n",
      "\n",
      "======================================================================\n",
      "FINAL VALUE COUNTS:\n",
      "======================================================================\n",
      "default_profile\n",
      "False    21734\n",
      "True     15712\n",
      "Name: count, dtype: int64\n",
      "\n",
      "True: 15712 (41.96%)\n",
      "False: 21734 (58.04%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Badr\\AppData\\Local\\Temp\\ipykernel_15956\\2120525870.py:69: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['default_profile'] = df['default_profile'].fillna(mode_value)\n"
     ]
    }
   ],
   "source": [
    "#cleaning default_profiile\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CLEANING: default_profile\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nBEFORE CLEANING:\")\n",
    "print(f\"Data type: {df['default_profile'].dtype}\")\n",
    "print(f\"Unique values: {df['default_profile'].nunique()}\")\n",
    "print(\"\\nValue counts:\")\n",
    "print(df['default_profile'].value_counts(dropna=False))\n",
    "\n",
    "# lowercase\n",
    "df['default_profile'] = df['default_profile'].astype(str).str.lower().str.strip()\n",
    "\n",
    "# map\n",
    "def map_to_boolean(value):\n",
    "    # if true\n",
    "    if value in ['true', '1', 'yes', 'eurt']:\n",
    "        return True\n",
    "    #if flase\n",
    "    elif value in ['false', '0', 'no', 'eslaf']:  # 'eslaf' is 'false' backwards\n",
    "        return False\n",
    "    # missing\n",
    "    elif value in ['nan', 'none', 'unknown', '-9999']:\n",
    "        return np.nan\n",
    "    #else = nan\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "df['default_profile'] = df['default_profile'].apply(map_to_boolean)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"AFTER CLEANING:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Data type: {df['default_profile'].dtype}\")\n",
    "print(f\"Unique values: {df['default_profile'].nunique()}\")\n",
    "print(\"\\nValue counts:\")\n",
    "print(df['default_profile'].value_counts(dropna=False))\n",
    "\n",
    "# Calculate how many were converted to NaN\n",
    "missing_after = df['default_profile'].isnull().sum()\n",
    "print(f\"\\nTotal missing/invalid values: {missing_after} ({missing_after/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Show what was mapped to NaN (for verification)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLEANING SUMMARY:\")\n",
    "print(\"=\"*70)\n",
    "print(\" Standardized 'True'/'TRUE'/'true' → True\")\n",
    "print(\" Standardized 'False'/'FALSE'/'false' → False\")\n",
    "print(\"Mapped 'eurT' (backwards) → True\")\n",
    "print(\" Mapped 'eslaF' (backwards) → False\")\n",
    "print(\"Set invalid values (numbers, random strings) → NaN\")\n",
    "\n",
    "print(f\"\\nRows with True: {(df['default_profile'] == True).sum()}\")\n",
    "print(f\"Rows with False: {(df['default_profile'] == False).sum()}\")\n",
    "print(f\"Rows with NaN: {df['default_profile'].isna().sum()}\")\n",
    "\n",
    "# impute missing values\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HANDLING MISSING VALUES:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Missing values before imputation: {df['default_profile'].isna().sum()}\")\n",
    "\n",
    "# Get the most common value (mode)\n",
    "mode_value = df['default_profile'].mode()[0]\n",
    "print(f\"Most common value (mode): {mode_value}\")\n",
    "\n",
    "# Impute NaN with mode\n",
    "df['default_profile'] = df['default_profile'].fillna(mode_value)\n",
    "\n",
    "print(f\"Missing values after imputation: {df['default_profile'].isna().sum()}\")\n",
    "print(\"All missing values imputed with False\")\n",
    "\n",
    "# Final value counts\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL VALUE COUNTS:\")\n",
    "print(\"=\"*70)\n",
    "print(df['default_profile'].value_counts(dropna=False))\n",
    "print(f\"\\nTrue: {(df['default_profile'] == True).sum()} ({(df['default_profile'] == True).sum()/len(df)*100:.2f}%)\")\n",
    "print(f\"False: {(df['default_profile'] == False).sum()} ({(df['default_profile'] == False).sum()/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa742b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CLEANING: default_profile_image\n",
      "======================================================================\n",
      "\n",
      "BEFORE CLEANING:\n",
      "Data type: object\n",
      "Unique values: 14\n",
      "\n",
      "Value counts:\n",
      "default_profile_image\n",
      "False            32243\n",
      "False             4615\n",
      "True               495\n",
      "True                63\n",
      "NaN                  9\n",
      "FALSE                4\n",
      "0                    4\n",
      "eslaF                4\n",
      "-9999                2\n",
      "INVALID_False        2\n",
      "JuaP6                1\n",
      "GHOdo                1\n",
      "unknown              1\n",
      "999999               1\n",
      "lsUB6                1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "======================================================================\n",
      "AFTER CLEANING:\n",
      "======================================================================\n",
      "Data type: object\n",
      "Unique values: 2\n",
      "\n",
      "Value counts:\n",
      "default_profile_image\n",
      "False    36872\n",
      "True       558\n",
      "NaN         16\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total missing/invalid values: 16 (0.04%)\n",
      "\n",
      "======================================================================\n",
      "HANDLING MISSING VALUES:\n",
      "======================================================================\n",
      "Missing values before imputation: 16\n",
      "Most common value (mode): False\n",
      "Missing values after imputation: 0\n",
      "✓ All missing values imputed with False\n",
      "\n",
      "======================================================================\n",
      "FINAL VALUE COUNTS:\n",
      "======================================================================\n",
      "default_profile_image\n",
      "False    36888\n",
      "True       558\n",
      "Name: count, dtype: int64\n",
      "\n",
      "True: 558 (1.49%)\n",
      "False: 36888 (98.51%)\n",
      "\n",
      "======================================================================\n",
      "CLEANING SUMMARY:\n",
      "======================================================================\n",
      "✓ Standardized 'True'/'TRUE'/'true' → True\n",
      "✓ Standardized 'False'/'FALSE'/'false' → False\n",
      "✓ Mapped 'eslaF' (backwards) → False\n",
      "✓ Mapped 'INVALID_False' → False\n",
      "✓ Set invalid values (numbers, random strings) → NaN\n",
      "✓ Imputed all NaN with mode (False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Badr\\AppData\\Local\\Temp\\ipykernel_15956\\3490893910.py:56: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['default_profile_image'] = df['default_profile_image'].fillna(mode_value)\n"
     ]
    }
   ],
   "source": [
    "# Clean the default_profile_image column\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CLEANING: default_profile_image\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nBEFORE CLEANING:\")\n",
    "print(f\"Data type: {df['default_profile_image'].dtype}\")\n",
    "print(f\"Unique values: {df['default_profile_image'].nunique()}\")\n",
    "print(\"\\nValue counts:\")\n",
    "print(df['default_profile_image'].value_counts(dropna=False))\n",
    "\n",
    "# lowercase\n",
    "df['default_profile_image'] = df['default_profile_image'].astype(str).str.lower().str.strip()\n",
    "\n",
    "# mapping\n",
    "def map_to_boolean(value):\n",
    "    #  True \n",
    "    if value in ['true', '1', 'yes', 'eurt']:  \n",
    "        return True\n",
    "    #  False \n",
    "    elif value in ['false', '0', 'no', 'eslaf', 'invalid_false']:  \n",
    "        return False\n",
    "    #  missing/invalid \n",
    "    elif value in ['nan', 'none', 'unknown', '-9999', '999999']:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "df['default_profile_image'] = df['default_profile_image'].apply(map_to_boolean)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"AFTER CLEANING:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Data type: {df['default_profile_image'].dtype}\")\n",
    "print(f\"Unique values: {df['default_profile_image'].nunique()}\")\n",
    "print(\"\\nValue counts:\")\n",
    "print(df['default_profile_image'].value_counts(dropna=False))\n",
    "\n",
    "# nan number\n",
    "missing_after = df['default_profile_image'].isnull().sum()\n",
    "print(f\"\\nTotal missing/invalid values: {missing_after} ({missing_after/len(df)*100:.2f}%)\")\n",
    "\n",
    "# imputing\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HANDLING MISSING VALUES:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Missing values before imputation: {df['default_profile_image'].isna().sum()}\")\n",
    "\n",
    "# Get the most common value (mode)\n",
    "mode_value = df['default_profile_image'].mode()[0]\n",
    "print(f\"Most common value (mode): {mode_value}\")\n",
    "\n",
    "# Impute NaN with mode\n",
    "df['default_profile_image'] = df['default_profile_image'].fillna(mode_value)\n",
    "\n",
    "print(f\"Missing values after imputation: {df['default_profile_image'].isna().sum()}\")\n",
    "print(f\"✓ All missing values imputed with {mode_value}\")\n",
    "\n",
    "# Final value counts\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL VALUE COUNTS:\")\n",
    "print(\"=\"*70)\n",
    "print(df['default_profile_image'].value_counts(dropna=False))\n",
    "print(f\"\\nTrue: {(df['default_profile_image'] == True).sum()} ({(df['default_profile_image'] == True).sum()/len(df)*100:.2f}%)\")\n",
    "print(f\"False: {(df['default_profile_image'] == False).sum()} ({(df['default_profile_image'] == False).sum()/len(df)*100:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d27c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CLEANING: verified\n",
      "======================================================================\n",
      "\n",
      "BEFORE CLEANING:\n",
      "Data type: object\n",
      "Unique values: 12\n",
      "\n",
      "Value counts:\n",
      "verified\n",
      "False            29861\n",
      "True              7545\n",
      "NaN                 19\n",
      "FALSE                5\n",
      "eslaF                4\n",
      "INVALID_False        3\n",
      "INVALID_True         2\n",
      "eurT                 2\n",
      "999999               1\n",
      "61q3S                1\n",
      "OtnX                 1\n",
      "-9999                1\n",
      "unknown              1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "======================================================================\n",
      "AFTER MAPPING:\n",
      "======================================================================\n",
      "Data type: object\n",
      "Unique values: 2\n",
      "\n",
      "Value counts:\n",
      "verified\n",
      "False    29873\n",
      "True      7549\n",
      "NaN         24\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total missing/invalid values: 24 (0.06%)\n",
      "\n",
      "======================================================================\n",
      "HANDLING MISSING VALUES:\n",
      "======================================================================\n",
      "Missing values before imputation: 24\n",
      "Most common value (mode): False\n",
      "Missing values after imputation: 0\n",
      "✓ All missing values imputed with False\n",
      "\n",
      "======================================================================\n",
      "FINAL VALUE COUNTS:\n",
      "======================================================================\n",
      "verified\n",
      "False    29897\n",
      "True      7549\n",
      "Name: count, dtype: int64\n",
      "\n",
      "True: 7549 (20.16%)\n",
      "False: 29897 (79.84%)\n",
      "\n",
      "======================================================================\n",
      "CLEANING SUMMARY:\n",
      "======================================================================\n",
      "✓ Standardized 'True'/'FALSE'/'false' → True/False\n",
      "✓ Mapped 'eslaF' (backwards) and 'INVALID_False' → False\n",
      "✓ Mapped 'eurT' and 'INVALID_True' → True\n",
      "✓ Set invalid values (numbers, random strings) → NaN (Total: 34 values)\n",
      "✓ Imputed all NaN with mode (False)\n",
      "✓ Final Data Type: bool\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Badr\\AppData\\Local\\Temp\\ipykernel_15956\\3464289228.py:64: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['verified'] = df['verified'].fillna(mode_value)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Clean the verified column\n",
    "print(\"=\"*70)\n",
    "print(\"CLEANING: verified\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nBEFORE CLEANING:\")\n",
    "print(f\"Data type: {df['verified'].dtype}\")\n",
    "print(f\"Unique values: {df['verified'].nunique()}\")\n",
    "print(\"\\nValue counts:\")\n",
    "print(df['verified'].value_counts(dropna=False))\n",
    "\n",
    "# lowercase\n",
    "df['verified'] = df['verified'].astype(str).str.lower().str.strip()\n",
    "\n",
    "# mapping\n",
    "def map_to_boolean(value):\n",
    "    if value in ['true', '1', 'yes', 'eurt', 'invalid_true', 'eurt']:\n",
    "        return True\n",
    "   \n",
    "    elif value in ['false', '0', 'no', 'eslaf', 'invalid_false']:\n",
    "        return False\n",
    "    \n",
    "    elif value in ['nan', 'none', 'unknown', '-9999', '999999', '61q3s', 'otnx']:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "df['verified'] = df['verified'].apply(map_to_boolean)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"AFTER MAPPING:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Data type: {df['verified'].dtype}\")\n",
    "print(f\"Unique values: {df['verified'].nunique()}\")\n",
    "print(\"\\nValue counts:\")\n",
    "print(df['verified'].value_counts(dropna=False))\n",
    "\n",
    "# Calculate how many were converted to NaN (the total count of 'np.nan')\n",
    "missing_after = df['verified'].isnull().sum()\n",
    "print(f\"\\nTotal missing/invalid values: {missing_after} ({missing_after/len(df)*100:.2f}%)\")\n",
    "\n",
    "# impute missing values\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HANDLING MISSING VALUES:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Missing values before imputation: {df['verified'].isna().sum()}\")\n",
    "\n",
    "mode_value = df['verified'].mode()[0]\n",
    "print(f\"Most common value (mode): {mode_value}\")\n",
    "\n",
    "df['verified'] = df['verified'].fillna(mode_value)\n",
    "df['verified'] = df['verified'].astype(bool)\n",
    "\n",
    "print(f\"Missing values after imputation: {df['verified'].isna().sum()}\")\n",
    "print(f\"✓ All missing values imputed with {mode_value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL VALUE COUNTS:\")\n",
    "print(\"=\"*70)\n",
    "print(df['verified'].value_counts(dropna=False))\n",
    "print(f\"\\nTrue: {(df['verified'] == True).sum()} ({(df['verified'] == True).sum()/len(df)*100:.2f}%)\")\n",
    "print(f\"False: {(df['verified'] == False).sum()} ({(df['verified'] == False).sum()/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Show what was cleaned\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLEANING SUMMARY:\")\n",
    "print(\"=\"*70)\n",
    "print(\"✓ Standardized 'True'/'FALSE'/'false' → True/False\")\n",
    "print(\"✓ Mapped 'eslaF' (backwards) and 'INVALID_False' → False\")\n",
    "print(\"✓ Mapped 'eurT' and 'INVALID_True' → True\")\n",
    "print(\"✓ Set invalid values (numbers, random strings) → NaN (Total: 34 values)\")\n",
    "print(f\"✓ Imputed all NaN with mode ({mode_value})\")\n",
    "print(f\"✓ Final Data Type: {df['verified'].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695d6050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CLEANING: geo_enabled\n",
      "======================================================================\n",
      "\n",
      "BEFORE CLEANING:\n",
      "Data type: object\n",
      "Unique values: 16\n",
      "\n",
      "Value counts:\n",
      "geo_enabled\n",
      "False                                                20343\n",
      "True                                                 17058\n",
      "NaN                                                     13\n",
      "eslaF                                                    7\n",
      "TRUE                                                     4\n",
      "eurT                                                     3\n",
      "unknown                                                  3\n",
      "999999                                                   3\n",
      "FALSE                                                    2\n",
      "-9999                                                    2\n",
      "http://abs.twimg.com/images/themes/theme1/bg.png         2\n",
      "http://abs.twimg.com/images/themes/theme10/bg.gif        1\n",
      "QHAQ                                                     1\n",
      "INVALID_False                                            1\n",
      "INVALID_True                                             1\n",
      "Lh1F                                                     1\n",
      "KnR4                                                     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "======================================================================\n",
      "AFTER MAPPING:\n",
      "======================================================================\n",
      "Data type: object\n",
      "Unique values: 2\n",
      "\n",
      "Value counts:\n",
      "geo_enabled\n",
      "False    20353\n",
      "True     17066\n",
      "NaN         27\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total missing/invalid values: 27 (0.07%)\n",
      "\n",
      "======================================================================\n",
      "HANDLING MISSING VALUES:\n",
      "======================================================================\n",
      "Missing values before imputation: 27\n",
      "Most common value (mode): False\n",
      "Missing values after imputation: 0\n",
      "✓ All missing values imputed with False\n",
      "\n",
      "======================================================================\n",
      "FINAL VALUE COUNTS:\n",
      "======================================================================\n",
      "geo_enabled\n",
      "False    20380\n",
      "True     17066\n",
      "Name: count, dtype: int64\n",
      "\n",
      "True: 17066 (45.57%)\n",
      "False: 20380 (54.43%)\n",
      "\n",
      "======================================================================\n",
      "CLEANING SUMMARY:\n",
      "======================================================================\n",
      "✓ Standardized 'True'/'FALSE'/'false' variations → True/False\n",
      "✓ Mapped 'eslaF' and 'INVALID_False' → False\n",
      "✓ Mapped 'eurT' and 'INVALID_True' → True\n",
      "✓ Set invalid values (URLs, numbers, random strings) → NaN (Total: 40 values)\n",
      "✓ Imputed all NaN with mode (False)\n",
      "✓ Final Data Type: bool\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Badr\\AppData\\Local\\Temp\\ipykernel_15956\\3447776333.py:69: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['geo_enabled'] = df['geo_enabled'].fillna(mode_value)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Clean the geo_enabled column\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CLEANING: geo_enabled\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nBEFORE CLEANING:\")\n",
    "print(f\"Data type: {df['geo_enabled'].dtype}\")\n",
    "print(f\"Unique values: {df['geo_enabled'].nunique()}\")\n",
    "print(\"\\nValue counts:\")\n",
    "print(df['geo_enabled'].value_counts(dropna=False))\n",
    "\n",
    "# lowercase\n",
    "df['geo_enabled'] = df['geo_enabled'].astype(str).str.lower().str.strip()\n",
    "\n",
    "# Step 2: Map all variations to proper boolean values\n",
    "def map_to_boolean_geo(value):\n",
    "   \n",
    "    if value in ['true', '1', 'yes', 'eurt', 'invalid_true']:\n",
    "        return True\n",
    "    \n",
    "    elif value in ['false', '0', 'no', 'eslaf', 'invalid_false']:\n",
    "        return False\n",
    "    \n",
    "    elif value in ['nan', 'none', 'unknown', '-9999', '999999', \n",
    "                    'http://abs.twimg.com/images/themes/theme1/bg.png', \n",
    "                    'http://abs.twimg.com/images/themes/theme10/bg.gif', \n",
    "                    'qhaq', 'lh1f', 'knr4']:\n",
    "        return np.nan\n",
    "    \n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "df['geo_enabled'] = df['geo_enabled'].apply(map_to_boolean_geo)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"AFTER MAPPING:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Data type: {df['geo_enabled'].dtype}\")\n",
    "print(f\"Unique values: {df['geo_enabled'].nunique()}\")\n",
    "print(\"\\nValue counts:\")\n",
    "print(df['geo_enabled'].value_counts(dropna=False))\n",
    "\n",
    "missing_after = df['geo_enabled'].isnull().sum()\n",
    "print(f\"\\nTotal missing/invalid values: {missing_after} ({missing_after/len(df)*100:.2f}%)\")\n",
    "\n",
    "\n",
    "#imputing\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HANDLING MISSING VALUES:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Missing values before imputation: {df['geo_enabled'].isna().sum()}\")\n",
    "mode_value = df['geo_enabled'].mode()[0]\n",
    "print(f\"Most common value (mode): {mode_value}\")\n",
    "\n",
    "df['geo_enabled'] = df['geo_enabled'].fillna(mode_value)\n",
    "\n",
    "df['geo_enabled'] = df['geo_enabled'].astype(bool)\n",
    "\n",
    "print(f\"Missing values after imputation: {df['geo_enabled'].isna().sum()}\")\n",
    "print(f\"All missing values imputed with {mode_value}\")\n",
    "\n",
    "# Final value counts\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL VALUE COUNTS:\")\n",
    "print(\"=\"*70)\n",
    "print(df['geo_enabled'].value_counts(dropna=False))\n",
    "print(f\"\\nTrue: {(df['geo_enabled'] == True).sum()} ({(df['geo_enabled'] == True).sum()/len(df)*100:.2f}%)\")\n",
    "print(f\"False: {(df['geo_enabled'] == False).sum()} ({(df['geo_enabled'] == False).sum()/len(df)*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab1da3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "🔍 INSPECTING NON-STANDARD VALUES IN NUMERIC COLUMNS\n",
      "======================================================================\n",
      "Finding all unique non-numeric strings or invalid placeholders:\n",
      "\n",
      "--- favourites_count (4 unique garbage/placeholder values) ---\n",
      "{'dV', 'INVALID_1754', 'fVR', 'unknown'}\n",
      "\n",
      "--- followers_count (10 unique garbage/placeholder values) ---\n",
      "{'INVALID_719', 'gKw', 'unknown', '-9999', 'INVALID_10', '-9999.0', 'en', 'INVALID_913', 'nI1naKB', 'ar'}\n",
      "\n",
      "--- friends_count (12 unique garbage/placeholder values) ---\n",
      "{'5Ej', 'where destination be.', '47.41155,8.543795', 'unknown', '-9999', 'INVALID_507', 'INVALID_845', 'INVALID_2115', 'INVALID_425', 'INVALID_0', 'INVALID_337', 'New York'}\n",
      "\n",
      "--- statuses_count (7 unique garbage/placeholder values) ---\n",
      "{'human', 'unknown', '-9999', 'INVALID_308', 'INVALID_1843', 'rhEVa2', 'bot'}\n",
      "\n",
      "======================================================================\n",
      "SUMMARY: All Unique Identified Garbage Values (28 types):\n",
      "======================================================================\n",
      "{'gKw', 'unknown', '47.41155,8.543795', 'INVALID_10', 'INVALID_0', 'dV', 'INVALID_719', 'INVALID_1754', 'INVALID_2115', 'en', 'fVR', 'INVALID_913', 'INVALID_337', 'ar', 'INVALID_507', 'INVALID_1843', '-9999.0', 'INVALID_425', 'New York', '5Ej', 'where destination be.', 'human', '-9999', 'INVALID_308', 'INVALID_845', 'rhEVa2', 'bot', 'nI1naKB'}\n"
     ]
    }
   ],
   "source": [
    "# List of numeric columns that are currently objects\n",
    "numeric_cols_to_inspect = [\n",
    "    'favourites_count', \n",
    "    'followers_count', \n",
    "    'friends_count', \n",
    "    'statuses_count'\n",
    "]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"INSPECTING NON-STANDARD VALUES IN NUMERIC COLUMNS\")\n",
    "print(\"=\"*70)\n",
    "print(\"Finding all unique non-numeric strings or invalid placeholders:\")\n",
    "\n",
    "# Dictionary to store unique garbage values for each column\n",
    "garbage_values = {}\n",
    "total_garbage_count = 0\n",
    "\n",
    "for col in numeric_cols_to_inspect:\n",
    "    # Get all unique values for the column\n",
    "    unique_values = df[col].astype(str).str.strip().unique()\n",
    "    \n",
    "    # Attempt to convert each unique value to numeric. If it fails (is an error), it's garbage.\n",
    "    # We also check for the specific numeric placeholder '-9999'.\n",
    "    garbage_list = []\n",
    "    \n",
    "    for val in unique_values:\n",
    "        # Check if the value is NaN or if it successfully converts to a number\n",
    "        try:\n",
    "            # Check for the specific numeric placeholder\n",
    "            if float(val) == -9999.0:\n",
    "                garbage_list.append(val)\n",
    "            \n",
    "            # Check for other garbage values (like random strings)\n",
    "            # If conversion fails, the code jumps to the 'except ValueError' block below.\n",
    "            float(val)\n",
    "            \n",
    "        except ValueError:\n",
    "            # This block captures non-numeric strings like 'unknown', 'eslaF', etc.\n",
    "            if val.lower() != 'nan': # Exclude the string 'nan' from the report if it came from NaNs\n",
    "                garbage_list.append(val)\n",
    "\n",
    "    if garbage_list:\n",
    "        # Print only the garbage found in this column\n",
    "        print(f\"\\n--- {col} ({len(garbage_list)} unique garbage/placeholder values) ---\")\n",
    "        print(set(garbage_list))\n",
    "        \n",
    "        # Add to the running total (using a set to ensure unique values across all columns)\n",
    "        for g_val in garbage_list:\n",
    "            if g_val not in garbage_values:\n",
    "                garbage_values[g_val] = 0\n",
    "            garbage_values[g_val] += 1\n",
    "            total_garbage_count += 1\n",
    "\n",
    "if not garbage_values:\n",
    "    print(\"\\nNo further unique non-numeric garbage strings found beyond what 'pd.to_numeric(errors='coerce')' handles!\")\n",
    "\n",
    "# Print the final set of all unique garbage values found across all columns\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"SUMMARY: All Unique Identified Garbage Values ({len(garbage_values)} types):\")\n",
    "print(\"=\"*70)\n",
    "print(set(garbage_values.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac28b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CLEANING: friends_count\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "AFTER COERCION AND GARBAGE FIX:\n",
      "======================================================================\n",
      "Data type: float64\n",
      "Total missing/invalid values (NaNs): 28\n",
      "\n",
      "======================================================================\n",
      "RECALCULATING MEDIAN FOR IMPUTATION:\n",
      "======================================================================\n",
      "New Median (calculated from clean data): 296\n",
      "\n",
      "======================================================================\n",
      "IMPUTING MISSING VALUES:\n",
      "======================================================================\n",
      "Missing values before imputation: 28\n",
      "Missing values after imputation: 0\n",
      "✓ All missing values imputed with 296\n",
      "\n",
      "======================================================================\n",
      "FINAL CLEANING SUMMARY:\n",
      "======================================================================\n",
      "Final Data Type: int64\n",
      "Final Missing Values: 0\n",
      "\n",
      "✓ 'friends_count' column successfully cleaned and converted to 'int64'\n"
     ]
    }
   ],
   "source": [
    "# CLEANING: friends_count \n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CLEANING: friends_count\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# convert to numeric, all non numeric values ==> nan\n",
    "df['friends_count'] = pd.to_numeric(df['friends_count'], errors='coerce')\n",
    "\n",
    "\n",
    "# df.loc[df['friends_count'] <= -9999, 'friends_count'] = np.nan\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"AFTER COERCION AND GARBAGE FIX:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Data type: {df['friends_count'].dtype}\")\n",
    "missing_after_fix = df['friends_count'].isnull().sum()\n",
    "print(f\"Total missing/invalid values (NaNs): {missing_after_fix}\")\n",
    "\n",
    "# calculate median\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RECALCULATING MEDIAN FOR IMPUTATION:\")\n",
    "print(\"=\"*70)\n",
    "median_value = df['friends_count'].median()\n",
    "print(f\"New Median (calculated from clean data): {median_value:.0f}\")\n",
    "\n",
    "# Impute missing with median\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"IMPUTING MISSING VALUES:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Missing values before imputation: {missing_after_fix}\")\n",
    "df['friends_count'] = df['friends_count'].fillna(median_value)\n",
    "\n",
    "print(f\"Missing values after imputation: {df['friends_count'].isna().sum()}\")\n",
    "print(f\"✓ All missing values imputed with {median_value:.0f}\")\n",
    "\n",
    "\n",
    "# convert to int\n",
    "df['friends_count'] = df['friends_count'].astype('int64')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL CLEANING SUMMARY:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Final Data Type: {df['friends_count'].dtype}\")\n",
    "print(f\"Final Missing Values: {df['friends_count'].isnull().sum()}\")\n",
    "print(\"\\n✓ 'friends_count' column successfully cleaned and converted to 'int64'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad0efab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CLEANING: favourites_count\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "AFTER COERCION AND GARBAGE FIX:\n",
      "======================================================================\n",
      "Data type: float64\n",
      "Total missing/invalid values (NaNs): 18\n",
      "\n",
      "======================================================================\n",
      "RECALCULATING MEDIAN FOR IMPUTATION:\n",
      "======================================================================\n",
      "New Median (calculated from clean data): 2068\n",
      "\n",
      "======================================================================\n",
      "IMPUTING MISSING VALUES:\n",
      "======================================================================\n",
      "Missing values before imputation: 18\n",
      "Missing values after imputation: 0\n",
      "✓ All missing values imputed with 2068\n",
      "\n",
      "======================================================================\n",
      "FINAL CLEANING SUMMARY:\n",
      "======================================================================\n",
      "Final Data Type: int64\n",
      "Final Missing Values: 0\n",
      "\n",
      "✓ 'favourites_count' column successfully cleaned and converted to 'int64'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# CLEANING: favourites_count \n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CLEANING: favourites_count\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# convert to numeric, all non numeric values ==> nan\n",
    "df['favourites_count'] = pd.to_numeric(df['favourites_count'], errors='coerce')\n",
    "\n",
    "#df.loc[df['favourites_count'] <= -9999, 'favourites_count'] = np.nan\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"AFTER COERCION AND GARBAGE FIX:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Data type: {df['favourites_count'].dtype}\")\n",
    "missing_after_fix = df['favourites_count'].isnull().sum()\n",
    "print(f\"Total missing/invalid values (NaNs): {missing_after_fix}\")\n",
    "\n",
    "# meidon claculation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RECALCULATING MEDIAN FOR IMPUTATION:\")\n",
    "print(\"=\"*70)\n",
    "median_value = df['favourites_count'].median()\n",
    "print(f\"New Median (calculated from clean data): {median_value:.0f}\")\n",
    "\n",
    "# impute missing with median\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"IMPUTING MISSING VALUES:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Missing values before imputation: {missing_after_fix}\")\n",
    "\n",
    "df['favourites_count'] = df['favourites_count'].fillna(median_value)\n",
    "\n",
    "print(f\"Missing values after imputation: {df['favourites_count'].isna().sum()}\")\n",
    "print(f\"All missing values imputed with {median_value:.0f}\")\n",
    "\n",
    "\n",
    "#convert to int\n",
    "df['favourites_count'] = df['favourites_count'].astype('int64')\n",
    "\n",
    "# --- Final Inspection ---\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL CLEANING SUMMARY:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Final Data Type: {df['favourites_count'].dtype}\")\n",
    "print(f\"Final Missing Values: {df['favourites_count'].isnull().sum()}\")\n",
    "print(\"\\n✓ 'favourites_count' column successfully cleaned and converted to 'int64'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e309e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CLEANING: statuses_count\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "AFTER COERCION AND GARBAGE FIX:\n",
      "======================================================================\n",
      "Data type: float64\n",
      "Total missing/invalid values (NaNs): 26\n",
      "\n",
      "======================================================================\n",
      "RECALCULATING MEDIAN FOR IMPUTATION:\n",
      "======================================================================\n",
      "New Median (calculated from clean data): 4210\n",
      "\n",
      "======================================================================\n",
      "IMPUTING MISSING VALUES:\n",
      "======================================================================\n",
      "Missing values before imputation: 26\n",
      "Missing values after imputation: 0\n",
      "✓ All missing values imputed with 4210\n",
      "\n",
      "======================================================================\n",
      "FINAL CLEANING SUMMARY:\n",
      "======================================================================\n",
      "Final Data Type: int64\n",
      "Final Missing Values: 0\n",
      "\n",
      "✓ 'statuses_count' column successfully cleaned and converted to 'int64'\n"
     ]
    }
   ],
   "source": [
    "# CLEANING: statuses_count\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CLEANING: statuses_count\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# convert to numeric, all non numeric values ==> nan\n",
    "df['statuses_count'] = pd.to_numeric(df['statuses_count'], errors='coerce')\n",
    "\n",
    "# Step 2: Explicitly map the recognized numeric placeholders to NaN\n",
    "# We map any value that is -9999 or lower to catch the placeholder.\n",
    "df.loc[df['statuses_count'] <= -9999, 'statuses_count'] = np.nan\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"AFTER COERCION AND GARBAGE FIX:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Data type: {df['statuses_count'].dtype}\")\n",
    "missing_after_fix = df['statuses_count'].isnull().sum()\n",
    "print(f\"Total missing/invalid values (NaNs): {missing_after_fix}\")\n",
    "\n",
    "# median calculation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RECALCULATING MEDIAN FOR IMPUTATION:\")\n",
    "print(\"=\"*70)\n",
    "median_value = df['statuses_count'].median()\n",
    "print(f\"New Median (calculated from clean data): {median_value:.0f}\")\n",
    "\n",
    "# impute with median\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"IMPUTING MISSING VALUES:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Missing values before imputation: {missing_after_fix}\")\n",
    "df['statuses_count'] = df['statuses_count'].fillna(median_value)\n",
    "\n",
    "print(f\"Missing values after imputation: {df['statuses_count'].isna().sum()}\")\n",
    "print(f\" All missing values imputed with {median_value:.0f}\")\n",
    "\n",
    "\n",
    "#convert to int\n",
    "df['statuses_count'] = df['statuses_count'].astype('int64')\n",
    "\n",
    "# --- Final Inspection ---\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL CLEANING SUMMARY:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Final Data Type: {df['statuses_count'].dtype}\")\n",
    "print(f\"Final Missing Values: {df['statuses_count'].isnull().sum()}\")\n",
    "print(\"\\n✓ 'statuses_count' column successfully cleaned and converted to 'int64'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d11ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CLEANING: followers_count\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "AFTER COERCION AND GARBAGE FIX:\n",
      "======================================================================\n",
      "Data type: float64\n",
      "Total missing/invalid values (NaNs): 25\n",
      "\n",
      "======================================================================\n",
      "RECALCULATING MEDIAN FOR IMPUTATION:\n",
      "======================================================================\n",
      "New Median (calculated from clean data): 365\n",
      "\n",
      "======================================================================\n",
      "IMPUTING MISSING VALUES:\n",
      "======================================================================\n",
      "Missing values before imputation: 25\n",
      "Missing values after imputation: 0\n",
      "✓ All missing values imputed with 365\n",
      "\n",
      "======================================================================\n",
      "FINAL CLEANING SUMMARY:\n",
      "======================================================================\n",
      "Final Data Type: int64\n",
      "Final Missing Values: 0\n",
      "\n",
      "✓ 'followers_count' column successfully cleaned and converted to 'int64'\n"
     ]
    }
   ],
   "source": [
    "# CLEANING: followers_count \n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CLEANING: followers_count\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# convert to numeric, all non numeric values ==> nan\n",
    "\n",
    "df['followers_count'] = pd.to_numeric(df['followers_count'], errors='coerce')\n",
    "\n",
    "# map -9999 to nan\n",
    "df.loc[df['followers_count'] <= -9999, 'followers_count'] = np.nan\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"AFTER COERCION AND GARBAGE FIX:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Data type: {df['followers_count'].dtype}\")\n",
    "missing_after_fix = df['followers_count'].isnull().sum()\n",
    "print(f\"Total missing/invalid values (NaNs): {missing_after_fix}\")\n",
    "\n",
    "# calculate median\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RECALCULATING MEDIAN FOR IMPUTATION:\")\n",
    "print(\"=\"*70)\n",
    "median_value = df['followers_count'].median()\n",
    "print(f\"New Median (calculated from clean data): {median_value:.0f}\")\n",
    "\n",
    "# impute\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"IMPUTING MISSING VALUES:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Missing values before imputation: {missing_after_fix}\")\n",
    "df['followers_count'] = df['followers_count'].fillna(median_value)\n",
    "\n",
    "print(f\"Missing values after imputation: {df['followers_count'].isna().sum()}\")\n",
    "print(f\"✓ All missing values imputed with {median_value:.0f}\")\n",
    "\n",
    "\n",
    "#convert tot int\n",
    "df['followers_count'] = df['followers_count'].astype('int64')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL CLEANING SUMMARY:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Final Data Type: {df['followers_count'].dtype}\")\n",
    "print(f\"Final Missing Values: {df['followers_count'].isnull().sum()}\")\n",
    "print(\"\\n✓ 'followers_count' column successfully cleaned and converted to 'int64'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8820a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "🔍 INSPECTING NON-STANDARD VALUES IN TIME-BASED METRICS\n",
      "======================================================================\n",
      "Finding all unique non-numeric strings or invalid placeholders:\n",
      "\n",
      "--- average_tweets_per_day (5 unique garbage/placeholder values) ---\n",
      "{'unknown', 'INVALID_1.961', 'DujwPd', 'INVALID_1.021', 'INVALID_11.035'}\n",
      "\n",
      "--- account_age_days (9 unique garbage/placeholder values) ---\n",
      "{'FbTT', 'sMFE', 'XGp0', '-9999', '53m8', 'vIIz', 'INVALID_3033', 'INVALID_3490', 'i9ve'}\n",
      "\n",
      "======================================================================\n",
      "SUMMARY: All Unique Identified Garbage Values (14 types):\n",
      "======================================================================\n",
      "{'FbTT', 'sMFE', 'XGp0', 'unknown', '-9999', '53m8', 'INVALID_1.961', 'DujwPd', 'vIIz', 'INVALID_3033', 'INVALID_1.021', 'INVALID_11.035', 'INVALID_3490', 'i9ve'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# List of columns to inspect\n",
    "time_metrics_to_inspect = [\n",
    "    'average_tweets_per_day', \n",
    "    'account_age_days'\n",
    "]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"🔍 INSPECTING NON-STANDARD VALUES IN TIME-BASED METRICS\")\n",
    "print(\"=\"*70)\n",
    "print(\"Finding all unique non-numeric strings or invalid placeholders:\")\n",
    "\n",
    "garbage_values = {}\n",
    "\n",
    "for col in time_metrics_to_inspect:\n",
    "    unique_values = df[col].astype(str).str.strip().unique()\n",
    "    \n",
    "    garbage_list = []\n",
    "    \n",
    "    for val in unique_values:\n",
    "        if val.startswith('-9999'):\n",
    "            garbage_list.append(val)\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Check if the value is NaN\n",
    "            if val.lower() == 'nan':\n",
    "                continue\n",
    "            \n",
    "            # If conversion fails, the code jumps to the 'except ValueError' block below.\n",
    "            float(val)\n",
    "            \n",
    "        except ValueError:\n",
    "            # This block captures non-numeric strings like 'unknown', 'bot', etc.\n",
    "            garbage_list.append(val)\n",
    "\n",
    "    if garbage_list:\n",
    "        # Print only the garbage found in this column\n",
    "        print(f\"\\n--- {col} ({len(garbage_list)} unique garbage/placeholder values) ---\")\n",
    "        print(set(garbage_list))\n",
    "        \n",
    "        # Add to the running set of all garbage found\n",
    "        for g_val in garbage_list:\n",
    "            garbage_values[g_val] = garbage_values.get(g_val, 0) + 1\n",
    "\n",
    "# Print the final set of all unique garbage values found across all columns\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"SUMMARY: All Unique Identified Garbage Values ({len(garbage_values)} types):\")\n",
    "print(\"=\"*70)\n",
    "print(set(garbage_values.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fe4dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CLEANING: average_tweets_per_day\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "AFTER VALUE EXTRACTION AND COERCION:\n",
      "======================================================================\n",
      "Data type: float64\n",
      "Total missing/invalid values (NaNs): 24\n",
      "\n",
      "======================================================================\n",
      "RECALCULATING MEDIAN FOR IMPUTATION:\n",
      "======================================================================\n",
      "New Median (calculated from clean data): 1.488\n",
      "\n",
      "======================================================================\n",
      "IMPUTING MISSING VALUES:\n",
      "======================================================================\n",
      "Missing values before imputation: 24\n",
      "Missing values after imputation: 0\n",
      "✓ All missing values imputed with 1.488\n",
      "\n",
      "======================================================================\n",
      "FINAL CLEANING SUMMARY:\n",
      "======================================================================\n",
      "Final Data Type: float64\n",
      "Final Missing Values: 0\n",
      "\n",
      "✓ 'average_tweets_per_day' column successfully cleaned and converted to 'float64'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# CLEANING: average_tweets_per_day\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CLEANING: average_tweets_per_day\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# convert to string\n",
    "df['average_tweets_per_day'] = df['average_tweets_per_day'].astype(str).str.strip()\n",
    "\n",
    "# dealing with INVALID_ prefixes\n",
    "df.loc[df['average_tweets_per_day'].str.startswith('INVALID_', na=False), 'average_tweets_per_day'] = \\\n",
    "    df['average_tweets_per_day'].str.replace('INVALID_', '', regex=False)\n",
    "\n",
    "# convert to numeric, all non numeric values ==> nan\n",
    "df['average_tweets_per_day'] = pd.to_numeric(df['average_tweets_per_day'], errors='coerce')\n",
    "\n",
    "# if -9999 or lower, set to nan\n",
    "df.loc[df['average_tweets_per_day'] <= -9999, 'average_tweets_per_day'] = np.nan\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"AFTER VALUE EXTRACTION AND COERCION:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Data type: {df['average_tweets_per_day'].dtype}\")\n",
    "missing_after_fix = df['average_tweets_per_day'].isnull().sum()\n",
    "print(f\"Total missing/invalid values (NaNs): {missing_after_fix}\")\n",
    "\n",
    "#median\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RECALCULATING MEDIAN FOR IMPUTATION:\")\n",
    "print(\"=\"*70)\n",
    "median_value = df['average_tweets_per_day'].median()\n",
    "print(f\"New Median (calculated from clean data): {median_value:.3f}\")\n",
    "\n",
    "# impute missing with median\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"IMPUTING MISSING VALUES:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Missing values before imputation: {missing_after_fix}\")\n",
    "\n",
    "df['average_tweets_per_day'] = df['average_tweets_per_day'].fillna(median_value)\n",
    "\n",
    "print(f\"Missing values after imputation: {df['average_tweets_per_day'].isna().sum()}\")\n",
    "print(f\"✓ All missing values imputed with {median_value:.3f}\")\n",
    "\n",
    "\n",
    "# convert to float\n",
    "df['average_tweets_per_day'] = df['average_tweets_per_day'].astype('float64')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL CLEANING SUMMARY:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Final Data Type: {df['average_tweets_per_day'].dtype}\")\n",
    "print(f\"Final Missing Values: {df['average_tweets_per_day'].isnull().sum()}\")\n",
    "print(\"\\n✓ 'average_tweets_per_day' column successfully cleaned and converted to 'float64'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d74f269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CLEANING: account_age_days\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "AFTER VALUE EXTRACTION AND COERCION:\n",
      "======================================================================\n",
      "Data type: float64\n",
      "Total missing/invalid values (NaNs): 23\n",
      "\n",
      "======================================================================\n",
      "RECALCULATING MEDIAN FOR IMPUTATION:\n",
      "======================================================================\n",
      "New Median (calculated from clean data): 3219 days\n",
      "\n",
      "======================================================================\n",
      "IMPUTING MISSING VALUES:\n",
      "======================================================================\n",
      "Missing values before imputation: 23\n",
      "Missing values after imputation: 0\n",
      "✓ All missing values imputed with 3219 days\n",
      "\n",
      "======================================================================\n",
      "FINAL CLEANING SUMMARY:\n",
      "======================================================================\n",
      "Final Data Type: int64\n",
      "Final Missing Values: 0\n",
      "\n",
      "✓ 'account_age_days' column successfully cleaned and converted to 'int64'\n"
     ]
    }
   ],
   "source": [
    "# CLEANING: account_age_days \n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CLEANING: account_age_days\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# convert to string\n",
    "df['account_age_days'] = df['account_age_days'].astype(str).str.strip()\n",
    "\n",
    "# invalid_prefix handling\n",
    "df.loc[df['account_age_days'].str.startswith('INVALID_', na=False), 'account_age_days'] = \\\n",
    "    df['account_age_days'].str.replace('INVALID_', '', regex=False)\n",
    "\n",
    "# convert to numeric, all non numeric values ==> nan\n",
    "df['account_age_days'] = pd.to_numeric(df['account_age_days'], errors='coerce')\n",
    "\n",
    "#if -9999 or lower, set to nan\n",
    "df.loc[df['account_age_days'] <= -9999, 'account_age_days'] = np.nan\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"AFTER VALUE EXTRACTION AND COERCION:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Data type: {df['account_age_days'].dtype}\")\n",
    "missing_after_fix = df['account_age_days'].isnull().sum()\n",
    "print(f\"Total missing/invalid values (NaNs): {missing_after_fix}\")\n",
    "\n",
    "#median\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RECALCULATING MEDIAN FOR IMPUTATION:\")\n",
    "print(\"=\"*70)\n",
    "median_value = df['account_age_days'].median()\n",
    "print(f\"New Median (calculated from clean data): {median_value:.0f} days\")\n",
    "\n",
    "#median\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"IMPUTING MISSING VALUES:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Missing values before imputation: {missing_after_fix}\")\n",
    "\n",
    "df['account_age_days'] = df['account_age_days'].fillna(median_value)\n",
    "\n",
    "print(f\"Missing values after imputation: {df['account_age_days'].isna().sum()}\")\n",
    "print(f\"✓ All missing values imputed with {median_value:.0f} days\")\n",
    "\n",
    "\n",
    "df['account_age_days'] = df['account_age_days'].astype('int64')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL CLEANING SUMMARY:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Final Data Type: {df['account_age_days'].dtype}\")\n",
    "print(f\"Final Missing Values: {df['account_age_days'].isnull().sum()}\")\n",
    "print(\"\\n✓ 'account_age_days' column successfully cleaned and converted to 'int64'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68ed2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DROPPING COLUMNS: Unnamed: 0, created_at\n",
      "======================================================================\n",
      "✓ Columns successfully dropped.\n",
      "New DataFrame shape: (37446, 18)\n",
      "Remaining columns: ['default_profile', 'default_profile_image', 'description', 'favourites_count', 'followers_count', 'friends_count', 'geo_enabled', 'id', 'lang', 'location', 'profile_background_image_url', 'profile_image_url', 'screen_name', 'statuses_count', 'verified', 'average_tweets_per_day', 'account_age_days', 'account_type']\n"
     ]
    }
   ],
   "source": [
    "# List of columns to drop\n",
    "columns_to_drop = ['Unnamed: 0', 'created_at']\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"DROPPING COLUMNS: {', '.join(columns_to_drop)}\")\n",
    "print(\"=\"*70)\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "print(f\"✓ Columns successfully dropped.\")\n",
    "print(f\"New DataFrame shape: {df.shape}\")\n",
    "print(f\"Remaining columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91631aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "🔍 INSPECTING: screen_name\n",
      "======================================================================\n",
      "Total missing values (NaNs): 10\n",
      "Top values (to check for placeholder names):\n",
      "| screen_name    |   count |\n",
      "|:---------------|--------:|\n",
      "| nan            |      10 |\n",
      "| 999999         |       2 |\n",
      "| unknown        |       2 |\n",
      "| JoleonLescott  |       1 |\n",
      "| Ayat_140       |       1 |\n",
      "| carrieanninaba |       1 |\n",
      "| sherine        |       1 |\n",
      "| cordensmaureen |       1 |\n",
      "| GhamGraham     |       1 |\n",
      "| jainabdulaziz  |       1 |\n",
      "\n",
      "======================================================================\n",
      "NON-STANDARD NAMES (Invalid Chars, Too Short/Long):\n",
      "======================================================================\n",
      "Found 9 non-standard names.\n",
      "| screen_name             |   count |\n",
      "|:------------------------|--------:|\n",
      "| vc                      |       1 |\n",
      "| EW                      |       1 |\n",
      "| INVALID_IAMFRIMPONG26   |       1 |\n",
      "| INVALID_kimchiri        |       1 |\n",
      "| HP                      |       1 |\n",
      "| YG                      |       1 |\n",
      "| INVALID_CapsulaCinefila |       1 |\n",
      "| INVALID_woahjergi       |       1 |\n",
      "| INVALID_kekaneshrikant  |       1 |\n",
      "\n",
      "Obvious Placeholder Names:\n",
      "| screen_name   |   count |\n",
      "|:--------------|--------:|\n",
      "| nan           |      10 |\n",
      "| unknown       |       2 |\n",
      "======================================================================\n",
      "🔍 SCREEN NAME (Twitter Handle) for Row Index 465:\n",
      "======================================================================\n",
      "Row Index 465, Screen Name: nan\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#screen_name investigation\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"🔍 INSPECTING: screen_name\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Convert to string to handle NaNs and check basic counts\n",
    "df['screen_name'] = df['screen_name'].astype(str).str.strip()\n",
    "\n",
    "missing_count = (df['screen_name'].str.lower() == 'nan').sum()\n",
    "print(f\"Total missing values (NaNs): {missing_count}\")\n",
    "print(f\"Top values (to check for placeholder names):\")\n",
    "print(df['screen_name'].value_counts().head(10).to_markdown())\n",
    "\n",
    "#  Check for Non-Standard Characters and Length\n",
    "\n",
    "allowed_chars = r'[^a-zA-Z0-9_]'\n",
    "non_standard_names = df[\n",
    "    (df['screen_name'].str.contains(allowed_chars, regex=True, na=False)) |  \n",
    "    (df['screen_name'].str.len() < 3) |                                     # Too short (Twitter requires 4)\n",
    "    (df['screen_name'].str.len() > 15)                                    # Too long (Twitter limit is 15)\n",
    "]['screen_name'].value_counts()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NON-STANDARD NAMES (Invalid Chars, Too Short/Long):\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if not non_standard_names.empty:\n",
    "    print(f\"Found {non_standard_names.sum()} non-standard names.\")\n",
    "    print(non_standard_names.head(20).to_markdown())\n",
    "else:\n",
    "    print(\"No obvious non-standard names or invalid characters found.\")\n",
    "\n",
    "# Check for obvious placeholders like 'unknown'\n",
    "placeholder_names = df[df['screen_name'].str.lower().isin(['unknown', 'nan', 'test', 'user'])]['screen_name'].value_counts()\n",
    "print(\"\\nObvious Placeholder Names:\")\n",
    "print(placeholder_names.to_markdown())\n",
    "\n",
    "screen_name_465 = df['screen_name'].iloc[465]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"🔍 SCREEN NAME (Twitter Handle) for Row Index 465:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Row Index 465, Screen Name: {screen_name_465}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71995b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CLEANING: screen_name\n",
      "======================================================================\n",
      "Extracting valid names from 'INVALID_' prefixes...\n",
      "Imputing missing and generic placeholder values...\n",
      "Replacing purely numeric screen names with placeholder...\n",
      "\n",
      "======================================================================\n",
      "FINAL CLEANING SUMMARY:\n",
      "======================================================================\n",
      "Final Missing Values (Placeholder Count): 65\n",
      "Remaining purely numeric screen names: 0\n",
      "Sample values after cleaning: ['best_in_dumbest', 'CJRubinPhoto', 'SVGEGENT', 'TinkerVHELPK5', 'JoleonLescott']\n",
      "\n",
      "✓ 'screen_name' successfully cleaned and standardized.\n"
     ]
    }
   ],
   "source": [
    "# Define the placeholder to use for missing/invalid names\n",
    "MISSING_NAME_PLACEHOLDER = 'MissingName'\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CLEANING: screen_name\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Ensure all are strings and standardized\n",
    "df['screen_name'] = df['screen_name'].astype(str).str.strip()\n",
    "\n",
    "# 'INVALID_' prefixes\n",
    "print(\"Extracting valid names from 'INVALID_' prefixes...\")\n",
    "df.loc[df['screen_name'].str.startswith('INVALID_', na=False), 'screen_name'] = \\\n",
    "    df['screen_name'].str.replace('INVALID_', '', regex=False)\n",
    "\n",
    "#impute with placeholder name\n",
    "print(\"Imputing missing and generic placeholder values...\")\n",
    "placeholders_to_replace = ['nan', 'unknown', '999999']\n",
    "df.loc[df['screen_name'].str.lower().isin(placeholders_to_replace), 'screen_name'] = MISSING_NAME_PLACEHOLDER\n",
    "\n",
    "# replace numeric screen names with placeholder\n",
    "print(\"Replacing purely numeric screen names with placeholder...\")\n",
    "mask_numeric = df['screen_name'].str.match(r'^\\d+$', na=False)\n",
    "df.loc[mask_numeric, 'screen_name'] = MISSING_NAME_PLACEHOLDER\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL CLEANING SUMMARY:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Final Missing Values (Placeholder Count): {(df['screen_name'] == MISSING_NAME_PLACEHOLDER).sum()}\")\n",
    "print(f\"Remaining purely numeric screen names: {df['screen_name'].str.match(r'^\\d+$', na=False).sum()}\")\n",
    "print(f\"Sample values after cleaning: {list(df['screen_name'].head())}\")\n",
    "print(\"\\n✓ 'screen_name' successfully cleaned and standardized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bae1a714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "🔍 INSPECTING: description (Text Column)\n",
      "======================================================================\n",
      "Original Missing Values (NaN): 7271\n",
      "\n",
      "Top 20 Value Counts (Case-Insensitive):\n",
      "| description_str                                                                       |   count |\n",
      "|:--------------------------------------------------------------------------------------|--------:|\n",
      "| nan                                                                                   |    7271 |\n",
      "| actor                                                                                 |      17 |\n",
      "| blacklivesmatter                                                                      |      16 |\n",
      "| .                                                                                     |      14 |\n",
      "| hi                                                                                    |      10 |\n",
      "| experience is one thing you can't get for nothing.                                    |       7 |\n",
      "| i am not afraid of death; i just don't want to be there when it happens.              |       7 |\n",
      "| :)                                                                                    |       7 |\n",
      "| black lives matter                                                                    |       6 |\n",
      "| fashion designer                                                                      |       5 |\n",
      "| one man's folly is another man's wife.                                                |       5 |\n",
      "| l                                                                                     |       5 |\n",
      "| it is one of the blessings of old friends that you can afford to be stupid with them. |       5 |\n",
      "| dream manfully and nobly; and thy dreams shall be prophets.                           |       5 |\n",
      "| artist                                                                                |       5 |\n",
      "| she/her                                                                               |       5 |\n",
      "| film maker                                                                            |       4 |\n",
      "| ♡                                                                                     |       4 |\n",
      "| designer                                                                              |       4 |\n",
      "| happy                                                                                 |       4 |\n",
      "\n",
      "Specific Placeholder Counts:\n",
      " - '.' count: 14\n",
      " - '-' count: 1\n",
      " - 'unknown' count: 1\n",
      " - 'nan' count: 7271\n",
      "\n",
      "Descriptions containing URLs (These are useful signals!): 4134\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# description\n",
    "# ======================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"🔍 INSPECTING: description (Text Column)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Handle Missing Values\n",
    "# First, count all the different ways 'missing' shows up (NaN, empty string, etc.)\n",
    "# Convert to string and strip for unified comparison\n",
    "df['description_str'] = df['description'].astype(str).str.strip()\n",
    "\n",
    "# Check for NaN (original missing values)\n",
    "original_nan_count = df['description'].isnull().sum()\n",
    "print(f\"Original Missing Values (NaN): {original_nan_count}\")\n",
    "\n",
    "# Check for common textual placeholders\n",
    "print(\"\\nTop 20 Value Counts (Case-Insensitive):\")\n",
    "# Use value_counts on the string column, mapping all to lowercase for better grouping\n",
    "description_counts = df['description_str'].str.lower().value_counts(dropna=False)\n",
    "print(description_counts.head(20).to_markdown())\n",
    "\n",
    "# Check for placeholders like a single dot, underscore, etc.\n",
    "placeholders_to_check = ['.', '-', 'none', 'unknown', 'nan', '']\n",
    "print(\"\\nSpecific Placeholder Counts:\")\n",
    "for p in placeholders_to_check:\n",
    "    count = (df['description_str'].str.lower() == p).sum()\n",
    "    if count > 0:\n",
    "        print(f\" - '{p}' count: {count}\")\n",
    "\n",
    "# Check for names that are simply URLs or seem like accidental data entry\n",
    "url_like_descriptions = df[df['description_str'].str.lower().str.contains('http|www', na=False)].shape[0]\n",
    "print(f\"\\nDescriptions containing URLs (These are useful signals!): {url_like_descriptions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5dac7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CLEANING: description\n",
      "======================================================================\n",
      "Original Missing Values (NaN): 7271\n",
      "✓ 7271 NaN values imputed with an empty string ('').\n",
      "\n",
      "======================================================================\n",
      "FINAL CLEANING SUMMARY:\n",
      "======================================================================\n",
      "Final Missing Values (NaN count): 0\n",
      "Sample values after cleaning: ['Blame xaiax, Inspired by MakingInvisible, using cmu phonetic data to produce incongruous matches.\\nSome images via Lorem Flickr.', 'Photographing the American West since 1980. I specialize in location portraits & events, both indoors & outside, using natural light & portable studio lighting.', 'Scruffy looking nerf herder and twitch broadcaster\\n\\nContact  svgegentgmail.com', 'Wife.Godmother.Friend.Feline Fanatic! Assistant Principal, Vestavia Hills Elementary Liberty Park', 'Loan coach at mancity & Aspiring DJ']\n",
      "\n",
      "✓ 'description' column successfully cleaned for feature engineering.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# CLEANING: description \n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CLEANING: description\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "#Impute NaN  with an empty string ('')\n",
    "original_nan_count = df['description'].isnull().sum()\n",
    "df['description'] = df['description'].fillna('')\n",
    "\n",
    "print(f\"Original Missing Values (NaN): {original_nan_count}\")\n",
    "print(f\"✓ {original_nan_count} NaN values imputed with an empty string ('').\")\n",
    "\n",
    "# ensure the column isstring \n",
    "df['description'] = df['description'].astype(str)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL CLEANING SUMMARY:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Final Missing Values (NaN count): {df['description'].isnull().sum()}\")\n",
    "print(f\"Sample values after cleaning: {list(df['description'].head().str.strip())}\")\n",
    "print(\"\\n✓ 'description' column successfully cleaned for feature engineering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc01603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate IDs: 14\n",
      "\n",
      "Duplicate rows based on 'id':\n",
      "       default_profile  default_profile_image  \\\n",
      "465               True                  False   \n",
      "466              False                  False   \n",
      "5574             False                  False   \n",
      "5575             False                  False   \n",
      "11119            False                  False   \n",
      "\n",
      "                                             description  favourites_count  \\\n",
      "465                                                                   2068   \n",
      "466                                                                   2068   \n",
      "5574                      EL SEOR ES MI REFUGIO ETERNO !              2068   \n",
      "5575                                                                  2068   \n",
      "11119  Military beat washingtonpost. UMassJournalism ...               181   \n",
      "\n",
      "       followers_count  friends_count  geo_enabled   id lang  \\\n",
      "465                365            296        False  NaN  NaN   \n",
      "466                365            296        False  NaN  NaN   \n",
      "5574               365            296        False  NaN  NaN   \n",
      "5575               365            296        False  NaN  NaN   \n",
      "11119            45714           3453         True  NaN   en   \n",
      "\n",
      "                location                      profile_background_image_url  \\\n",
      "465                  NaN                                               NaN   \n",
      "466                  NaN                                               NaN   \n",
      "5574                 NaN                                               NaN   \n",
      "5575                 NaN                                               NaN   \n",
      "11119  Northern Virginia  http://abs.twimg.com/images/themes/theme1/bg.png   \n",
      "\n",
      "                                       profile_image_url  screen_name  \\\n",
      "465                                                  NaN  MissingName   \n",
      "466                                                  NaN  MissingName   \n",
      "5574                                                 NaN  MissingName   \n",
      "5575                                                 NaN  MissingName   \n",
      "11119  http://pbs.twimg.com/profile_images/9312962078...   DanLamothe   \n",
      "\n",
      "       statuses_count  verified  average_tweets_per_day  account_age_days  \\\n",
      "465              4209     False                   1.488              3219   \n",
      "466              4209     False                   1.488              3219   \n",
      "5574             4209     False                   1.488              3219   \n",
      "5575             4209     False                   1.488              3219   \n",
      "11119           46179      True                  11.510              4012   \n",
      "\n",
      "      account_type                                    description_str  \n",
      "465            NaN  اللهم صلي على محد وعلى الي محمد كما صليت على إ...  \n",
      "466            NaN                                                nan  \n",
      "5574           NaN                    EL SEÑOR ES MI REFUGIO ETERNO !  \n",
      "5575           NaN                                                nan  \n",
      "11119        human  Military beat washingtonpost. UMassJournalism ...  \n"
     ]
    }
   ],
   "source": [
    "# Check how many duplicates exist in the 'id' column\n",
    "duplicate_count = df['id'].duplicated().sum()\n",
    "print(f\"Number of duplicate IDs: {duplicate_count}\")\n",
    "\n",
    "#  Display all duplicate rows \n",
    "duplicates = df[df['id'].duplicated(keep=False)]\n",
    "print(\"\\nDuplicate rows based on 'id':\")\n",
    "print(duplicates.head())\n",
    "\n",
    "#drop duplicates\n",
    "df_no_duplicates = df.drop_duplicates(subset='id', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c42b68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped columns: ['id', 'lang', 'location', 'profile_background_image_url', 'profile_image_url', 'description_str']\n",
      "\n",
      "New shape: (37446, 13)\n",
      "\n",
      "Remaining columns:\n",
      "['default_profile', 'default_profile_image', 'description', 'favourites_count', 'followers_count', 'friends_count', 'geo_enabled', 'screen_name', 'statuses_count', 'verified', 'average_tweets_per_day', 'account_age_days', 'account_type']\n"
     ]
    }
   ],
   "source": [
    "# drop\n",
    "columns_to_drop = [\n",
    "    'id',\n",
    "    'lang',\n",
    "    'location',\n",
    "    'profile_background_image_url',\n",
    "    'profile_image_url',\n",
    "    'description_str'\n",
    "]\n",
    "\n",
    "existing_cols = [col for col in columns_to_drop if col in df.columns]\n",
    "df = df.drop(columns=existing_cols)\n",
    "\n",
    "print(\"Dropped columns:\", existing_cols)\n",
    "print(\"\\nNew shape:\", df.shape)\n",
    "print(\"\\nRemaining columns:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e016c3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SAVING DATASET TO: bot_detection_cleaned.csv\n",
      "======================================================================\n",
      "✓ Dataset successfully saved to 'bot_detection_cleaned.csv'\n",
      "\n",
      "Your dataset is now safely stored with all quantitative features cleaned!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output_filename = 'bot_detection_cleaned.csv'\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"SAVING DATASET TO: {output_filename}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"✓ Dataset successfully saved to '{output_filename}'\")\n",
    "print(\"\\nYour dataset is now safely stored with all quantitative features cleaned!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0f1ab9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (37446, 13)\n",
      "\n",
      "Columns and types:\n",
      "default_profile              bool\n",
      "default_profile_image        bool\n",
      "description                object\n",
      "favourites_count            int64\n",
      "followers_count             int64\n",
      "friends_count               int64\n",
      "geo_enabled                  bool\n",
      "screen_name                object\n",
      "statuses_count              int64\n",
      "verified                     bool\n",
      "average_tweets_per_day    float64\n",
      "account_age_days            int64\n",
      "account_type               object\n",
      "dtype: object\n",
      "\n",
      "First few rows:\n",
      "   default_profile  default_profile_image  \\\n",
      "0            False                  False   \n",
      "1            False                  False   \n",
      "2            False                  False   \n",
      "3             True                  False   \n",
      "4            False                  False   \n",
      "\n",
      "                                         description  favourites_count  \\\n",
      "0  Blame xaiax, Inspired by MakingInvisible, usin...                 4   \n",
      "1  Photographing the American West since 1980. I ...               536   \n",
      "2  Scruffy looking nerf herder and twitch broadca...              3307   \n",
      "3  Wife.Godmother.Friend.Feline Fanatic! Assistan...              8433   \n",
      "4                Loan coach at mancity & Aspiring DJ                88   \n",
      "\n",
      "   followers_count  friends_count  geo_enabled      screen_name  \\\n",
      "0             1589              4        False  best_in_dumbest   \n",
      "1              860            880        False     CJRubinPhoto   \n",
      "2              172            594         True         SVGEGENT   \n",
      "3              517            633         True    TinkerVHELPK5   \n",
      "4           753678            116         True    JoleonLescott   \n",
      "\n",
      "   statuses_count  verified  average_tweets_per_day  account_age_days  \\\n",
      "0           11041     False                   7.870              1403   \n",
      "1             252     False                   0.183              1379   \n",
      "2            1001     False                   0.864              1159   \n",
      "3            1324     False                   0.889              1489   \n",
      "4            4202      True                   1.339              3138   \n",
      "\n",
      "  account_type  \n",
      "0          bot  \n",
      "1        human  \n",
      "2        human  \n",
      "3        human  \n",
      "4        human  \n",
      "\n",
      "Basic stats:\n",
      "       favourites_count  followers_count  friends_count  statuses_count  \\\n",
      "count      3.744600e+04     3.744600e+04   3.744600e+04    3.744600e+04   \n",
      "mean       2.339086e+13     3.701383e+05   4.471639e+03    2.106108e+04   \n",
      "std        4.526358e+15     2.470546e+06   4.980637e+04    6.731920e+04   \n",
      "min        0.000000e+00     0.000000e+00   0.000000e+00    0.000000e+00   \n",
      "25%        3.630000e+02     3.500000e+01   3.700000e+01    1.339000e+03   \n",
      "50%        2.068000e+03     3.650000e+02   2.960000e+02    4.209000e+03   \n",
      "75%        8.882000e+03     8.419250e+03   8.920000e+02    1.713950e+04   \n",
      "max        8.758941e+17     1.216415e+08   4.343060e+06    2.771910e+06   \n",
      "\n",
      "       average_tweets_per_day  account_age_days  \n",
      "count            37446.000000      37446.000000  \n",
      "mean                33.628615       3022.418496  \n",
      "std               5167.719953       1013.841042  \n",
      "min                  0.000000        483.000000  \n",
      "25%                  0.495000       2318.000000  \n",
      "50%                  1.488000       3219.000000  \n",
      "75%                  5.527000       3888.000000  \n",
      "max             999999.000000       8283.000000  \n",
      "\n",
      "Missing values:\n",
      "default_profile              0\n",
      "default_profile_image        0\n",
      "description               7714\n",
      "favourites_count             0\n",
      "followers_count              0\n",
      "friends_count                0\n",
      "geo_enabled                  0\n",
      "screen_name                  0\n",
      "statuses_count               0\n",
      "verified                     0\n",
      "average_tweets_per_day       0\n",
      "account_age_days             0\n",
      "account_type                21\n",
      "dtype: int64\n",
      "account_type\n",
      "human    25005\n",
      "bot      12420\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"bot_detection_cleaned.csv\")\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"\\nColumns and types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nBasic stats:\")\n",
    "print(df.describe())\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(df['account_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d88b0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INITIAL STATE\n",
      "============================================================\n",
      "Total rows: 37446\n",
      "Missing account_type: 21\n",
      "\n",
      "Unique values in account_type:\n",
      "account_type\n",
      "human    25005\n",
      "bot      12420\n",
      "NaN         21\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Checking for non-standard values:\n",
      "'bot' | Length: 3 | Type: <class 'str'>\n",
      "'human' | Length: 5 | Type: <class 'str'>\n",
      "============================================================\n",
      "FINAL DATASET\n",
      "============================================================\n",
      "Total rows: 37425\n",
      "Rows dropped: 21\n",
      "\n",
      "Class distribution:\n",
      "account_type\n",
      "human    25005\n",
      "bot      12420\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class balance: account_type\n",
      "human    0.668136\n",
      "bot      0.331864\n",
      "Name: proportion, dtype: float64\n",
      "Missing values: 0\n",
      "\n",
      "✓ Dataset ready for modeling\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"INITIAL STATE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Missing account_type: {df['account_type'].isna().sum()}\")\n",
    "print(f\"\\nUnique values in account_type:\")\n",
    "print(df['account_type'].value_counts(dropna=False))\n",
    "\n",
    "# Check for any weird whitespace or hidden characters\n",
    "print(\"\\nChecking for non-standard values:\")\n",
    "unique_vals = df['account_type'].unique()\n",
    "for val in unique_vals:\n",
    "    if pd.notna(val):\n",
    "        print(f\"'{val}' | Length: {len(str(val))} | Type: {type(val)}\")\n",
    "\n",
    "df_clean = df.dropna(subset=['account_type'])\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL DATASET\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total rows: {len(df_clean)}\")\n",
    "print(f\"Rows dropped: {len(df) - len(df_clean)}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df_clean['account_type'].value_counts())\n",
    "print(f\"\\nClass balance: {df_clean['account_type'].value_counts(normalize=True)}\")\n",
    "print(f\"Missing values: {df_clean['account_type'].isna().sum()}\")\n",
    "\n",
    "# Verify\n",
    "assert df_clean['account_type'].isna().sum() == 0, \"Still have missing labels!\"\n",
    "assert df_clean['account_type'].isin(['human', 'bot']).all(), \"Invalid labels found!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc2150f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VALIDATING DATA AGAINST ACCOUNT AGE\n",
      "============================================================\n",
      "\n",
      "1. CHECKING THEORETICAL LIMITS:\n",
      "------------------------------------------------------------\n",
      "Accounts with MORE tweets than physically possible (2400/day): 0\n",
      "\n",
      "\n",
      "2. VALIDATING AVERAGE_TWEETS_PER_DAY CALCULATION:\n",
      "------------------------------------------------------------\n",
      "Rows where average_tweets_per_day doesn't match calculation: 47\n",
      "\n",
      "Sample of mismatches:\n",
      "        screen_name  statuses_count  account_age_days  average_tweets_per_day  \\\n",
      "233     kweenklarke            4209              2760                  30.890   \n",
      "262       JeniBohls            1884              3126                   1.488   \n",
      "349     TomFletcher            4209              4154                   9.787   \n",
      "1497      mpolletta           34502              3219                   8.191   \n",
      "1714     BudMiller9            4209              2074                   0.765   \n",
      "2112  erikmackenzie           38174              6402                  18.658   \n",
      "2845        aRolasx           11844              3219                   3.459   \n",
      "3533        263Chat          351684              2860                   1.488   \n",
      "3644       USATODAY          326301              4395                   1.488   \n",
      "3710       faberskj            4209              3000                  46.229   \n",
      "\n",
      "      calculated_tweets_per_day  tweets_per_day_diff  \n",
      "233                    1.525000            29.365000  \n",
      "262                    0.602687             0.885313  \n",
      "349                    1.013240             8.773760  \n",
      "1497                  10.718235             2.527235  \n",
      "1714                   2.029412             1.264412  \n",
      "2112                   5.962824            12.695176  \n",
      "2845                   3.679404             0.220404  \n",
      "3533                 122.966434           121.478434  \n",
      "3644                  74.243686            72.755686  \n",
      "3710                   1.403000            44.826000  \n",
      "\n",
      "\n",
      "3. ANALYZING CAPPED VALUES (999,999):\n",
      "------------------------------------------------------------\n",
      "Accounts with tweets_per_day = 999,999: 1\n",
      "\n",
      "These accounts in detail:\n",
      "     screen_name  statuses_count  account_age_days  average_tweets_per_day  calculated_tweets_per_day account_type\n",
      "9683   jowee0661            3337              2826                999999.0                   1.180821          bot\n",
      "\n",
      "\n",
      "Recalculated tweets_per_day for capped accounts:\n",
      "  jowee0661: Should be 1.18 tweets/day (not 999,999)\n",
      "\n",
      "\n",
      "4. VALIDATING FAVOURITES_COUNT:\n",
      "------------------------------------------------------------\n",
      "Accounts with MORE favorites than theoretically possible: 0\n",
      "\n",
      "\n",
      "5. REALISTIC HUMAN LIMITS:\n",
      "------------------------------------------------------------\n",
      "Accounts exceeding realistic human tweet rate (>50/day avg): 792\n",
      "  Bots: 387\n",
      "  Humans: 405\n",
      "\n",
      "Accounts exceeding realistic favorite rate (>100/day avg): 149\n",
      "  Bots: 24\n",
      "  Humans: 125\n",
      "\n",
      "\n",
      "============================================================\n",
      "DATA QUALITY SUMMARY\n",
      "============================================================\n",
      "                              Issue  Count Percentage\n",
      "  Physically impossible tweet count      0      0.00%\n",
      "            Tweets_per_day mismatch     47      0.13%\n",
      "    Capped tweets_per_day (999,999)      1      0.00%\n",
      "               Impossible favorites      0      0.00%\n",
      "    Suspicious tweet rate (>50/day)    792      2.12%\n",
      "Suspicious favorite rate (>100/day)    149      0.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Badr\\AppData\\Local\\Temp\\ipykernel_15956\\4279845164.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean['theoretical_max_tweets'] = df_clean['account_age_days'] * 2400\n",
      "C:\\Users\\Badr\\AppData\\Local\\Temp\\ipykernel_15956\\4279845164.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean['calculated_tweets_per_day'] = df_clean['statuses_count'] / df_clean['account_age_days']\n",
      "C:\\Users\\Badr\\AppData\\Local\\Temp\\ipykernel_15956\\4279845164.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean['tweets_per_day_diff'] = abs(df_clean['average_tweets_per_day'] - df_clean['calculated_tweets_per_day'])\n",
      "C:\\Users\\Badr\\AppData\\Local\\Temp\\ipykernel_15956\\4279845164.py:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean['theoretical_max_favorites'] = df_clean['account_age_days'] * 10000\n",
      "C:\\Users\\Badr\\AppData\\Local\\Temp\\ipykernel_15956\\4279845164.py:83: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean['realistic_max_tweets'] = df_clean['account_age_days'] * 50\n",
      "C:\\Users\\Badr\\AppData\\Local\\Temp\\ipykernel_15956\\4279845164.py:84: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean['realistic_max_favorites'] = df_clean['account_age_days'] * 100\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"VALIDATING DATA AGAINST ACCOUNT AGE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate theoretical maximums based on account age\n",
    "print(\"\\n1. CHECKING THEORETICAL LIMITS:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Theoretical max tweets = account_age_days * tweets_per_day\n",
    "# Twitter rate limit: ~2400 tweets/day\n",
    "df_clean['theoretical_max_tweets'] = df_clean['account_age_days'] * 2400\n",
    "\n",
    "# Check if statuses_count exceeds what's physically possible\n",
    "impossible_tweets = df_clean['statuses_count'] > df_clean['theoretical_max_tweets']\n",
    "print(f\"Accounts with MORE tweets than physically possible (2400/day): {impossible_tweets.sum()}\")\n",
    "\n",
    "if impossible_tweets.sum() > 0:\n",
    "    print(\"\\nSample of impossible accounts:\")\n",
    "    print(df_clean[impossible_tweets][['screen_name', 'statuses_count', 'account_age_days', \n",
    "                                        'theoretical_max_tweets', 'average_tweets_per_day', \n",
    "                                        'account_type']].head(10))\n",
    "\n",
    "# 2. Validate average_tweets_per_day calculation\n",
    "print(\"\\n\\n2. VALIDATING AVERAGE_TWEETS_PER_DAY CALCULATION:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Recalculate what it SHOULD be\n",
    "df_clean['calculated_tweets_per_day'] = df_clean['statuses_count'] / df_clean['account_age_days']\n",
    "\n",
    "# Compare with existing value\n",
    "df_clean['tweets_per_day_diff'] = abs(df_clean['average_tweets_per_day'] - df_clean['calculated_tweets_per_day'])\n",
    "\n",
    "# Check for mismatches (allowing small floating point differences)\n",
    "mismatches = df_clean['tweets_per_day_diff'] > 0.01\n",
    "print(f\"Rows where average_tweets_per_day doesn't match calculation: {mismatches.sum()}\")\n",
    "\n",
    "if mismatches.sum() > 0:\n",
    "    print(\"\\nSample of mismatches:\")\n",
    "    print(df_clean[mismatches][['screen_name', 'statuses_count', 'account_age_days',\n",
    "                                 'average_tweets_per_day', 'calculated_tweets_per_day',\n",
    "                                 'tweets_per_day_diff']].head(10))\n",
    "\n",
    "# 3. Check the 999,999 values in context of account age\n",
    "print(\"\\n\\n3. ANALYZING CAPPED VALUES (999,999):\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "capped_tweets_per_day = df_clean[df_clean['average_tweets_per_day'] == 999999]\n",
    "print(f\"Accounts with tweets_per_day = 999,999: {len(capped_tweets_per_day)}\")\n",
    "\n",
    "if len(capped_tweets_per_day) > 0:\n",
    "    print(\"\\nThese accounts in detail:\")\n",
    "    print(capped_tweets_per_day[['screen_name', 'statuses_count', 'account_age_days',\n",
    "                                  'average_tweets_per_day', 'calculated_tweets_per_day',\n",
    "                                  'account_type']].to_string())\n",
    "    \n",
    "    # What SHOULD their tweets_per_day be?\n",
    "    print(\"\\n\\nRecalculated tweets_per_day for capped accounts:\")\n",
    "    for idx, row in capped_tweets_per_day.iterrows():\n",
    "        actual_rate = row['statuses_count'] / row['account_age_days']\n",
    "        print(f\"  {row['screen_name']}: Should be {actual_rate:.2f} tweets/day (not 999,999)\")\n",
    "\n",
    "# 4. Check favourites_count against activity\n",
    "print(\"\\n\\n4. VALIDATING FAVOURITES_COUNT:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# You can't favorite more tweets than you could possibly see\n",
    "# maybe 10,000 tweets/day max to see/favorite\n",
    "df_clean['theoretical_max_favorites'] = df_clean['account_age_days'] * 10000\n",
    "\n",
    "impossible_favs = df_clean['favourites_count'] > df_clean['theoretical_max_favorites']\n",
    "print(f\"Accounts with MORE favorites than theoretically possible: {impossible_favs.sum()}\")\n",
    "\n",
    "if impossible_favs.sum() > 0:\n",
    "    print(\"\\nSample of impossible favorite counts:\")\n",
    "    print(df_clean[impossible_favs][['screen_name', 'favourites_count', 'account_age_days',\n",
    "                                      'theoretical_max_favorites', 'account_type']].head(10))\n",
    "\n",
    "# 5. More realistic checks for human behavior\n",
    "print(\"\\n\\n5. REALISTIC HUMAN LIMITS:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Humans rarely tweet more than 50/day consistently, favorites >100/day is very high\n",
    "df_clean['realistic_max_tweets'] = df_clean['account_age_days'] * 50\n",
    "df_clean['realistic_max_favorites'] = df_clean['account_age_days'] * 100\n",
    "\n",
    "suspicious_tweet_rate = df_clean['statuses_count'] > df_clean['realistic_max_tweets']\n",
    "suspicious_fav_rate = df_clean['favourites_count'] > df_clean['realistic_max_favorites']\n",
    "\n",
    "print(f\"Accounts exceeding realistic human tweet rate (>50/day avg): {suspicious_tweet_rate.sum()}\")\n",
    "print(f\"  Bots: {(suspicious_tweet_rate & (df_clean['account_type'] == 'bot')).sum()}\")\n",
    "print(f\"  Humans: {(suspicious_tweet_rate & (df_clean['account_type'] == 'human')).sum()}\")\n",
    "\n",
    "print(f\"\\nAccounts exceeding realistic favorite rate (>100/day avg): {suspicious_fav_rate.sum()}\")\n",
    "print(f\"  Bots: {(suspicious_fav_rate & (df_clean['account_type'] == 'bot')).sum()}\")\n",
    "print(f\"  Humans: {(suspicious_fav_rate & (df_clean['account_type'] == 'human')).sum()}\")\n",
    "\n",
    "# 6. Summary of data quality issues\n",
    "print(\"\\n\\n\" + \"=\"*60)\n",
    "print(\"DATA QUALITY SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "issues = pd.DataFrame({\n",
    "    'Issue': [\n",
    "        'Physically impossible tweet count',\n",
    "        'Tweets_per_day mismatch',\n",
    "        'Capped tweets_per_day (999,999)',\n",
    "        'Impossible favorites',\n",
    "        'Suspicious tweet rate (>50/day)',\n",
    "        'Suspicious favorite rate (>100/day)'\n",
    "    ],\n",
    "    'Count': [\n",
    "        impossible_tweets.sum(),\n",
    "        mismatches.sum(),\n",
    "        (df_clean['average_tweets_per_day'] == 999999).sum(),\n",
    "        impossible_favs.sum(),\n",
    "        suspicious_tweet_rate.sum(),\n",
    "        suspicious_fav_rate.sum()\n",
    "    ],\n",
    "    'Percentage': [\n",
    "        f\"{impossible_tweets.sum()/len(df_clean)*100:.2f}%\",\n",
    "        f\"{mismatches.sum()/len(df_clean)*100:.2f}%\",\n",
    "        f\"{(df_clean['average_tweets_per_day'] == 999999).sum()/len(df_clean)*100:.2f}%\",\n",
    "        f\"{impossible_favs.sum()/len(df_clean)*100:.2f}%\",\n",
    "        f\"{suspicious_tweet_rate.sum()/len(df_clean)*100:.2f}%\",\n",
    "        f\"{suspicious_fav_rate.sum()/len(df_clean)*100:.2f}%\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(issues.to_string(index=False))\n",
    "\n",
    "# Clean up temporary columns\n",
    "df_clean = df_clean.drop(['theoretical_max_tweets', 'theoretical_max_favorites',\n",
    "                          'realistic_max_tweets', 'realistic_max_favorites',\n",
    "                          'calculated_tweets_per_day', 'tweets_per_day_diff'], \n",
    "                         axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fdd03477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FIXING DATA QUALITY ISSUES\n",
      "============================================================\n",
      "\n",
      "1. Recalculating average_tweets_per_day...\n",
      "   ✓ Fixed calculation\n",
      "   Rows with tweets_per_day > 1000 (should be 0): 4\n",
      "\n",
      "\n",
      "2. Analyzing high-activity accounts...\n",
      "------------------------------------------------------------\n",
      "\n",
      "Accounts with >50 tweets/day: 792\n",
      "  Bots: 387\n",
      "  Humans: 405\n",
      "\n",
      "Tweet rate distribution for high-activity accounts:\n",
      "count     792.000000\n",
      "mean      110.474686\n",
      "std       121.682492\n",
      "min        50.016619\n",
      "25%        59.621075\n",
      "50%        76.006541\n",
      "75%       111.286427\n",
      "max      1269.054924\n",
      "Name: average_tweets_per_day, dtype: float64\n",
      "\n",
      "Sample of high-activity accounts:\n",
      "           screen_name  average_tweets_per_day  statuses_count  account_age_days  followers_count account_type\n",
      "37234       10printbot             1269.054924         1340122              1056               15          bot\n",
      "19527         sectest9             1191.285891         1925118              1616            32444          bot\n",
      "8150   IndieGameDevBot             1080.593721         2305987              2134            44260          bot\n",
      "527          piketebow             1062.034483         2771910              2610              738          bot\n",
      "8343    EveryFinnishNo              969.163934         2069165              2135              196          bot\n",
      "10845   khalidrafiq138              968.014659         2113176              2183              966          bot\n",
      "560       TickerReport              791.999599         1974455              2493             3388          bot\n",
      "32994      godtributes              783.042958         1768111              2258            25206          bot\n",
      "34837     joaquinbala_              776.448728         2014108              2594              806          bot\n",
      "13631    DinheiRonaldo              676.969331         1346492              1989              912          bot\n",
      "\n",
      "\n",
      "============================================================\n",
      "RECOMMENDATION: APPLY IQR METHOD\n",
      "============================================================\n",
      "\n",
      "Now that we've validated the data is physically possible:\n",
      "1. ✓ Fixed the calculation errors (47 rows)\n",
      "2. ✓ Fixed the 999,999 cap (1 row)\n",
      "3. High activity accounts (>50/day) are legitimate but may be outliers\n",
      "\n",
      "Next step: Apply IQR method to handle statistical outliers\n",
      "- These high-activity accounts will naturally be flagged as outliers\n",
      "- IQR will handle them systematically across ALL numeric features\n",
      "\n",
      "\n",
      "4. Saving corrected average_tweets_per_day...\n",
      "\n",
      "✓ Data validation and correction complete!\n",
      "Total rows: 37425\n",
      "\n",
      "Ready to apply IQR method for outlier detection.\n",
      "\n",
      "============================================================\n",
      "CORRECTED STATISTICS\n",
      "============================================================\n",
      "       favourites_count  followers_count  friends_count  statuses_count  \\\n",
      "count      37425.000000     3.742500e+04   3.742500e+04    3.742500e+04   \n",
      "mean       12331.589259     3.703405e+05   4.473558e+03    2.106924e+04   \n",
      "std        34312.023211     2.471224e+06   4.982024e+04    6.733705e+04   \n",
      "min            0.000000     0.000000e+00   0.000000e+00    0.000000e+00   \n",
      "25%          362.000000     3.500000e+01   3.700000e+01    1.338000e+03   \n",
      "50%         2068.000000     3.650000e+02   2.960000e+02    4.209000e+03   \n",
      "75%         8879.000000     8.425000e+03   8.920000e+02    1.714400e+04   \n",
      "max       999999.000000     1.216415e+08   4.343060e+06    2.771910e+06   \n",
      "\n",
      "       average_tweets_per_day  \n",
      "count            37425.000000  \n",
      "mean                 6.890332  \n",
      "std                 24.583196  \n",
      "min                  0.000000  \n",
      "25%                  0.495348  \n",
      "50%                  1.487988  \n",
      "75%                  5.523934  \n",
      "max               1269.054924  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FIXING DATA QUALITY ISSUES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. FIX: Recalculate average_tweets_per_day for all rows\n",
    "print(\"\\n1. Recalculating average_tweets_per_day...\")\n",
    "df_clean['average_tweets_per_day'] = df_clean['statuses_count'] / df_clean['account_age_days']\n",
    "\n",
    "# Verify the fix\n",
    "mismatches_after = (df_clean['average_tweets_per_day'] > 1000).sum()\n",
    "print(f\"   ✓ Fixed calculation\")\n",
    "print(f\"   Rows with tweets_per_day > 1000 (should be 0): {mismatches_after}\")\n",
    "\n",
    "# 2. DECISION: What to do with high-activity accounts?\n",
    "print(\"\\n\\n2. Analyzing high-activity accounts...\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Let's see the distribution of tweet rates by account type\n",
    "high_tweet_rate = df_clean[df_clean['average_tweets_per_day'] > 50].copy()\n",
    "print(f\"\\nAccounts with >50 tweets/day: {len(high_tweet_rate)}\")\n",
    "print(f\"  Bots: {(high_tweet_rate['account_type'] == 'bot').sum()}\")\n",
    "print(f\"  Humans: {(high_tweet_rate['account_type'] == 'human').sum()}\")\n",
    "\n",
    "print(\"\\nTweet rate distribution for high-activity accounts:\")\n",
    "print(high_tweet_rate['average_tweets_per_day'].describe())\n",
    "\n",
    "# Sample some to see if they look legitimate\n",
    "print(\"\\nSample of high-activity accounts:\")\n",
    "sample = high_tweet_rate.nlargest(10, 'average_tweets_per_day')[\n",
    "    ['screen_name', 'average_tweets_per_day', 'statuses_count', \n",
    "     'account_age_days', 'followers_count', 'account_type']\n",
    "]\n",
    "print(sample.to_string())\n",
    "\n",
    "# 3. RECOMMENDATION: Use IQR now that data is validated\n",
    "print(\"\\n\\n\" + \"=\"*60)\n",
    "print(\"RECOMMENDATION: APPLY IQR METHOD\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "Now that we've validated the data is physically possible:\n",
    "1. ✓ Fixed the calculation errors (47 rows)\n",
    "2. ✓ Fixed the 999,999 cap (1 row)\n",
    "3. High activity accounts (>50/day) are legitimate but may be outliers\n",
    "\n",
    "Next step: Apply IQR method to handle statistical outliers\n",
    "- These high-activity accounts will naturally be flagged as outliers\n",
    "- IQR will handle them systematically across ALL numeric features\n",
    "\"\"\")\n",
    "\n",
    "# Save the corrected data\n",
    "print(\"\\n4. Saving corrected average_tweets_per_day...\")\n",
    "# df_clean.to_csv('data_with_corrected_tweets_per_day.csv', index=False)\n",
    "\n",
    "print(\"\\n✓ Data validation and correction complete!\")\n",
    "print(f\"Total rows: {len(df_clean)}\")\n",
    "print(\"\\nReady to apply IQR method for outlier detection.\")\n",
    "\n",
    "# Quick preview of corrected stats\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CORRECTED STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(df_clean[['favourites_count', 'followers_count', 'friends_count',\n",
    "                'statuses_count', 'average_tweets_per_day']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f526fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============================================================\n",
      "APPLYING IQR METHOD ON CORRECTED DATA\n",
      "============================================================\n",
      "favourites_count:\n",
      "  Upper bound: 21,654.50\n",
      "  Outliers capped: 5193 (13.88%)\n",
      "followers_count:\n",
      "  Upper bound: 21,010.00\n",
      "  Outliers capped: 8202 (21.92%)\n",
      "friends_count:\n",
      "  Upper bound: 2,174.50\n",
      "  Outliers capped: 4102 (10.96%)\n",
      "statuses_count:\n",
      "  Upper bound: 40,853.00\n",
      "  Outliers capped: 4507 (12.04%)\n",
      "average_tweets_per_day:\n",
      "  Upper bound: 13.07\n",
      "  Outliers capped: 4527 (12.10%)\n",
      "\n",
      "============================================================\n",
      "FINAL CLEANED DATASET\n",
      "============================================================\n",
      "Total rows: 37425\n",
      "\n",
      "Class distribution:\n",
      "account_type\n",
      "human    25005\n",
      "bot      12420\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Final statistics:\n",
      "       favourites_count  followers_count  friends_count  statuses_count  \\\n",
      "count      37425.000000     37425.000000   37425.000000    37425.000000   \n",
      "mean        5923.321456      5521.356232     604.899158    11442.270701   \n",
      "std         7694.700205      8604.946855     723.230814    13935.328725   \n",
      "min            0.000000         0.000000       0.000000        0.000000   \n",
      "25%          362.000000        35.000000      37.000000     1338.000000   \n",
      "50%         2068.000000       365.000000     296.000000     4209.000000   \n",
      "75%         8879.000000      8425.000000     892.000000    17144.000000   \n",
      "max        21654.500000     21010.000000    2174.500000    40853.000000   \n",
      "\n",
      "       average_tweets_per_day  \n",
      "count            37425.000000  \n",
      "mean                 3.708710  \n",
      "std                  4.425699  \n",
      "min                  0.000000  \n",
      "25%                  0.495348  \n",
      "50%                  1.487988  \n",
      "75%                  5.523934  \n",
      "max                 13.066814  \n"
     ]
    }
   ],
   "source": [
    "# Continue with IQR method on corrected data\n",
    "print(\"\\n\\n\" + \"=\"*60)\n",
    "print(\"APPLYING IQR METHOD ON CORRECTED DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "numeric_cols = ['favourites_count', 'followers_count', 'friends_count', \n",
    "                'statuses_count', 'average_tweets_per_day']\n",
    "\n",
    "def apply_iqr_capping(df, columns, multiplier=1.5):\n",
    "    \"\"\"Cap outliers using IQR method\"\"\"\n",
    "    df_capped = df.copy()\n",
    "    \n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = max(0, Q1 - multiplier * IQR)\n",
    "        upper_bound = Q3 + multiplier * IQR\n",
    "        \n",
    "        # Count outliers before capping\n",
    "        outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n",
    "        \n",
    "        # Cap values\n",
    "        df_capped[col] = df_capped[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "        \n",
    "        print(f\"{col}:\")\n",
    "        print(f\"  Upper bound: {upper_bound:,.2f}\")\n",
    "        print(f\"  Outliers capped: {outliers} ({outliers/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    return df_capped\n",
    "\n",
    "\n",
    "df_final = apply_iqr_capping(df_clean, numeric_cols, multiplier=1.5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL CLEANED DATASET\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total rows: {len(df_final)}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df_final['account_type'].value_counts())\n",
    "print(f\"\\nFinal statistics:\")\n",
    "print(df_final[numeric_cols].describe())\n",
    "\n",
    "# Save final cleaned data\n",
    "df_final.to_csv('final_cleaned_twitter_bot_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e11d1c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (37425, 13)\n",
      "\n",
      "Columns and types:\n",
      "default_profile              bool\n",
      "default_profile_image        bool\n",
      "description                object\n",
      "favourites_count          float64\n",
      "followers_count             int64\n",
      "friends_count             float64\n",
      "geo_enabled                  bool\n",
      "screen_name                object\n",
      "statuses_count              int64\n",
      "verified                     bool\n",
      "average_tweets_per_day    float64\n",
      "account_age_days            int64\n",
      "account_type               object\n",
      "dtype: object\n",
      "\n",
      "First few rows:\n",
      "   default_profile  default_profile_image  \\\n",
      "0            False                  False   \n",
      "1            False                  False   \n",
      "2            False                  False   \n",
      "3             True                  False   \n",
      "4            False                  False   \n",
      "\n",
      "                                         description  favourites_count  \\\n",
      "0  Blame xaiax, Inspired by MakingInvisible, usin...               4.0   \n",
      "1  Photographing the American West since 1980. I ...             536.0   \n",
      "2  Scruffy looking nerf herder and twitch broadca...            3307.0   \n",
      "3  Wife.Godmother.Friend.Feline Fanatic! Assistan...            8433.0   \n",
      "4                Loan coach at mancity & Aspiring DJ              88.0   \n",
      "\n",
      "   followers_count  friends_count  geo_enabled      screen_name  \\\n",
      "0             1589            4.0        False  best_in_dumbest   \n",
      "1              860          880.0        False     CJRubinPhoto   \n",
      "2              172          594.0         True         SVGEGENT   \n",
      "3              517          633.0         True    TinkerVHELPK5   \n",
      "4            21010          116.0         True    JoleonLescott   \n",
      "\n",
      "   statuses_count  verified  average_tweets_per_day  account_age_days  \\\n",
      "0           11041     False                7.869565              1403   \n",
      "1             252     False                0.182741              1379   \n",
      "2            1001     False                0.863676              1159   \n",
      "3            1324     False                0.889187              1489   \n",
      "4            4202      True                1.339069              3138   \n",
      "\n",
      "  account_type  \n",
      "0          bot  \n",
      "1        human  \n",
      "2        human  \n",
      "3        human  \n",
      "4        human  \n",
      "\n",
      "Basic stats:\n",
      "       favourites_count  followers_count  friends_count  statuses_count  \\\n",
      "count      37425.000000     37425.000000   37425.000000    37425.000000   \n",
      "mean        5923.321456      5521.356232     604.899158    11442.270701   \n",
      "std         7694.700205      8604.946855     723.230814    13935.328725   \n",
      "min            0.000000         0.000000       0.000000        0.000000   \n",
      "25%          362.000000        35.000000      37.000000     1338.000000   \n",
      "50%         2068.000000       365.000000     296.000000     4209.000000   \n",
      "75%         8879.000000      8425.000000     892.000000    17144.000000   \n",
      "max        21654.500000     21010.000000    2174.500000    40853.000000   \n",
      "\n",
      "       average_tweets_per_day  account_age_days  \n",
      "count            37425.000000      37425.000000  \n",
      "mean                 3.708710       3022.332077  \n",
      "std                  4.425699       1014.040772  \n",
      "min                  0.000000        483.000000  \n",
      "25%                  0.495348       2317.000000  \n",
      "50%                  1.487988       3219.000000  \n",
      "75%                  5.523934       3888.000000  \n",
      "max                 13.066814       8283.000000  \n",
      "\n",
      "Missing values:\n",
      "default_profile              0\n",
      "default_profile_image        0\n",
      "description               7706\n",
      "favourites_count             0\n",
      "followers_count              0\n",
      "friends_count                0\n",
      "geo_enabled                  0\n",
      "screen_name                  0\n",
      "statuses_count               0\n",
      "verified                     0\n",
      "average_tweets_per_day       0\n",
      "account_age_days             0\n",
      "account_type                 0\n",
      "dtype: int64\n",
      "account_type\n",
      "human    25005\n",
      "bot      12420\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_please_work = pd.read_csv(\"final_cleaned_twitter_bot_data.csv\")\n",
    "\n",
    "print(\"Shape:\", df_please_work.shape)\n",
    "print(\"\\nColumns and types:\")\n",
    "print(df_please_work.dtypes)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df_please_work.head())\n",
    "print(\"\\nBasic stats:\")\n",
    "print(df_please_work.describe())\n",
    "print(\"\\nMissing values:\")\n",
    "print(df_please_work.isnull().sum())\n",
    "\n",
    "print(df_please_work['account_type'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
